{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kalman Filter Multi-step Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Initial Setup\n",
    "Set directory to load log files from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rospkg\n",
    "\n",
    "# Create a RosPack object\n",
    "rospack = rospkg.RosPack()\n",
    "\n",
    "# Get the path to the package this script is in\n",
    "package_path = rospack.get_path('hri_predict_ros')\n",
    "\n",
    "# Define the path to the logs directory\n",
    "log_dir = os.path.join(package_path, 'logs')\n",
    "\n",
    "# Specify the path to the rosbag files and the gui data\n",
    "bag_dir = os.path.join(log_dir, 'bag')\n",
    "gui_dir = os.path.join(log_dir, 'gui_data')\n",
    "npz_dir = os.path.join(log_dir, 'npz')\n",
    "\n",
    "# Define the path to the plots directory\n",
    "plot_dir = os.path.join(package_path, 'plots')\n",
    "if not os.path.exists(plot_dir):\n",
    "    os.makedirs(plot_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import npz files in Pandas dataframes for each subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder 'sub_3' with 2449 npz files\n",
      "Processed subject sub_3. Number of rows: 2449\n",
      "\n",
      "Processing folder 'sub_4' with 2630 npz files\n",
      "Processed subject sub_4. Number of rows: 2630\n",
      "\n",
      "Processing folder 'sub_6' with 2694 npz files\n",
      "Processed subject sub_6. Number of rows: 2694\n",
      "\n",
      "Processing folder 'sub_7' with 3756 npz files\n",
      "Processed subject sub_7. Number of rows: 3756\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Get a list of all folders in npz_dir\n",
    "folders = [f for f in os.listdir(npz_dir) if os.path.isdir(os.path.join(npz_dir, f))]\n",
    "folders = sorted(folders, key=lambda s: int(s.split('_')[1]))\n",
    "\n",
    "folders = folders[:-1] # DEBUG\n",
    "\n",
    "# Create an empty dictionary to store the dataframes\n",
    "npz_dfs = {}\n",
    "\n",
    "# Iterate over each folder\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(npz_dir, folder)\n",
    "    \n",
    "    # Get a list of all npz files in the folder\n",
    "    npz_files = [f for f in os.listdir(folder_path) if f.endswith('.npz')]\n",
    "    sorted_npz_files = sorted(npz_files, key=lambda s: int(s.split('_')[1].split('.')[0]))\n",
    "\n",
    "    print(\"Processing folder '%s' with %d npz files\" % (folder, len(sorted_npz_files)))\n",
    "\n",
    "    df_all = []\n",
    "    \n",
    "    # Iterate over each npz file\n",
    "    for npz_file in sorted_npz_files:\n",
    "        npz_file_path = os.path.join(folder_path, npz_file)\n",
    "        \n",
    "        # Load the npz file\n",
    "        data = np.load(npz_file_path)\n",
    "\n",
    "        df= pd.DataFrame.from_dict({item: data[item] for item in data.files}, orient='index')\n",
    "        df = df.transpose()\n",
    "\n",
    "        # Append the dataframe to the list\n",
    "        df_all.append(df)\n",
    "\n",
    "    # Concatenate all the dataframes in the list\n",
    "    df_all = pd.concat(df_all, ignore_index=True)\n",
    "\n",
    "    print(\"Processed subject %s. Number of rows: %d\\n\" % (folder, len(df_all)))\n",
    "\n",
    "    # Store the dataframe in the dictionary\n",
    "    npz_dfs[folder] = df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import gui_data for each subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Get all the file names in the directory\n",
    "file_names = os.listdir(gui_dir)\n",
    "\n",
    "# DEBUG\n",
    "file_names = ['gui_log_sub_3.txt', 'gui_log_sub_4.txt', 'gui_log_sub_6.txt', 'gui_log_sub_7.txt']\n",
    "\n",
    "# Initialize an empty dictionary to store the dataframes\n",
    "gui_dfs = {}\n",
    "\n",
    "# Iterate over each file\n",
    "for file_name in file_names:\n",
    "    # Check if the file is a text file\n",
    "    if file_name.endswith('.txt'): # they are txt files, but structured as csv\n",
    "        # Construct the file path\n",
    "        file_path = os.path.join(gui_dir, file_name)\n",
    "        \n",
    "        # Read the file as a dataframe\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Add the dataframe to the dictionary using a portion of the file name as the key\n",
    "        key = file_name.split('.')[0].split('_')[-2:]\n",
    "        key = '_'.join(key)\n",
    "        gui_dfs[key] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create lists of column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kp0_x', 'kp0_xd', 'kp0_xdd', 'kp0_y', 'kp0_yd', 'kp0_ydd', 'kp0_z', 'kp0_zd', 'kp0_zdd', 'kp1_x', 'kp1_xd', 'kp1_xdd', 'kp1_y', 'kp1_yd', 'kp1_ydd', 'kp1_z', 'kp1_zd', 'kp1_zdd', 'kp2_x', 'kp2_xd', 'kp2_xdd', 'kp2_y', 'kp2_yd', 'kp2_ydd', 'kp2_z', 'kp2_zd', 'kp2_zdd', 'kp3_x', 'kp3_xd', 'kp3_xdd', 'kp3_y', 'kp3_yd', 'kp3_ydd', 'kp3_z', 'kp3_zd', 'kp3_zdd', 'kp4_x', 'kp4_xd', 'kp4_xdd', 'kp4_y', 'kp4_yd', 'kp4_ydd', 'kp4_z', 'kp4_zd', 'kp4_zdd', 'kp5_x', 'kp5_xd', 'kp5_xdd', 'kp5_y', 'kp5_yd', 'kp5_ydd', 'kp5_z', 'kp5_zd', 'kp5_zdd', 'kp6_x', 'kp6_xd', 'kp6_xdd', 'kp6_y', 'kp6_yd', 'kp6_ydd', 'kp6_z', 'kp6_zd', 'kp6_zdd', 'kp7_x', 'kp7_xd', 'kp7_xdd', 'kp7_y', 'kp7_yd', 'kp7_ydd', 'kp7_z', 'kp7_zd', 'kp7_zdd', 'kp8_x', 'kp8_xd', 'kp8_xdd', 'kp8_y', 'kp8_yd', 'kp8_ydd', 'kp8_z', 'kp8_zd', 'kp8_zdd', 'kp9_x', 'kp9_xd', 'kp9_xdd', 'kp9_y', 'kp9_yd', 'kp9_ydd', 'kp9_z', 'kp9_zd', 'kp9_zdd', 'kp10_x', 'kp10_xd', 'kp10_xdd', 'kp10_y', 'kp10_yd', 'kp10_ydd', 'kp10_z', 'kp10_zd', 'kp10_zdd', 'kp11_x', 'kp11_xd', 'kp11_xdd', 'kp11_y', 'kp11_yd', 'kp11_ydd', 'kp11_z', 'kp11_zd', 'kp11_zdd', 'kp12_x', 'kp12_xd', 'kp12_xdd', 'kp12_y', 'kp12_yd', 'kp12_ydd', 'kp12_z', 'kp12_zd', 'kp12_zdd', 'kp13_x', 'kp13_xd', 'kp13_xdd', 'kp13_y', 'kp13_yd', 'kp13_ydd', 'kp13_z', 'kp13_zd', 'kp13_zdd', 'kp14_x', 'kp14_xd', 'kp14_xdd', 'kp14_y', 'kp14_yd', 'kp14_ydd', 'kp14_z', 'kp14_zd', 'kp14_zdd', 'kp15_x', 'kp15_xd', 'kp15_xdd', 'kp15_y', 'kp15_yd', 'kp15_ydd', 'kp15_z', 'kp15_zd', 'kp15_zdd', 'kp16_x', 'kp16_xd', 'kp16_xdd', 'kp16_y', 'kp16_yd', 'kp16_ydd', 'kp16_z', 'kp16_zd', 'kp16_zdd', 'kp17_x', 'kp17_xd', 'kp17_xdd', 'kp17_y', 'kp17_yd', 'kp17_ydd', 'kp17_z', 'kp17_zd', 'kp17_zdd']\n",
      "162\n",
      "['kp0_x', 'kp0_y', 'kp0_z', 'kp1_x', 'kp1_y', 'kp1_z', 'kp2_x', 'kp2_y', 'kp2_z', 'kp3_x', 'kp3_y', 'kp3_z', 'kp4_x', 'kp4_y', 'kp4_z', 'kp5_x', 'kp5_y', 'kp5_z', 'kp6_x', 'kp6_y', 'kp6_z', 'kp7_x', 'kp7_y', 'kp7_z', 'kp8_x', 'kp8_y', 'kp8_z', 'kp9_x', 'kp9_y', 'kp9_z', 'kp10_x', 'kp10_y', 'kp10_z', 'kp11_x', 'kp11_y', 'kp11_z', 'kp12_x', 'kp12_y', 'kp12_z', 'kp13_x', 'kp13_y', 'kp13_z', 'kp14_x', 'kp14_y', 'kp14_z', 'kp15_x', 'kp15_y', 'kp15_z', 'kp16_x', 'kp16_y', 'kp16_z', 'kp17_x', 'kp17_y', 'kp17_z']\n",
      "54\n"
     ]
    }
   ],
   "source": [
    "n_keypoints = 18\n",
    "state_names = ['kp{}_{}'.format(i, suffix)\n",
    "               for i in range(n_keypoints)\n",
    "               for suffix in ['x', 'xd', 'xdd', 'y', 'yd', 'ydd', 'z', 'zd', 'zdd']]\n",
    "print(state_names)\n",
    "print(len(state_names))\n",
    "\n",
    "measurement_names = ['kp{}_{}'.format(i, suffix)\n",
    "                     for i in range(n_keypoints)\n",
    "                     for suffix in ['x', 'y', 'z']]\n",
    "print(measurement_names)\n",
    "print(len(measurement_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manipulate time series\n",
    "- Change the datatype for the timestamp from string to datetime\n",
    "- separate measured, filtered and predicted values in different dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 9\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# print(df.head())\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# NPZ DATAFRAMES (npz_dfs)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subj, df \u001b[38;5;129;01min\u001b[39;00m npz_dfs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# check if the timestamp column is present\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m:\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;66;03m# Convert the 'timestamp' column to a TimeDeltaIndex\u001b[39;00m\n\u001b[1;32m     11\u001b[0m         df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m], unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;66;03m# Rename 'timestamp' column to 'Timestamp'\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "# GUI_LOG DATAFRAMES (gui_dfs)\n",
    "for _, df in gui_dfs.items():\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "    # print(df.head())\n",
    "\n",
    "# NPZ DATAFRAMES (npz_dfs)\n",
    "final_npz_dfs = {}\n",
    "for subj, df in npz_dfs.items():\n",
    "    # check if the timestamp column is present\n",
    "    if 'timestamp' in df.columns:\n",
    "        # Convert the 'timestamp' column to a TimeDeltaIndex\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'][0], unit='s')\n",
    "        \n",
    "        # Rename 'timestamp' column to 'Timestamp'\n",
    "        df = df.rename(columns={'timestamp': 'Timestamp'})\n",
    "\n",
    "    df_meas = df[['Timestamp', 'human_meas_pos']]\n",
    "    df_filt_state = df[['Timestamp', 'human_filt_x']]\n",
    "    df_filt_var = df[['Timestamp', 'human_filt_var']]\n",
    "    df_pred_state = df[['Timestamp', 'pred_human_x']] # TEMP\n",
    "    df_pred_var = df[['Timestamp', 'pred_human_var']] # TEMP\n",
    "\n",
    "    print(type(df_pred_state['pred_human_x'][0]), df_pred_state['pred_human_x'][0].shape)\n",
    "\n",
    "    for c in df_meas.columns.values:\n",
    "        df_meas = pd.concat([df_meas, df_meas.pop(c).apply(pd.Series).add_prefix(c+\"_\")], axis=1)\n",
    "\n",
    "    for c in df_filt_state.columns.values:\n",
    "        df_filt_state = pd.concat([df_filt_state, df_filt_state.pop(c).apply(pd.Series).add_prefix(c+\"_\")], axis=1)\n",
    "\n",
    "    for c in df_filt_var.columns.values:\n",
    "        df_filt_var = pd.concat([df_filt_var, df_filt_var.pop(c).apply(pd.Series).add_prefix(c+\"_\")], axis=1)\n",
    "\n",
    "    # print(df_meas.columns.values)\n",
    "    # print(df_meas.shape)\n",
    "\n",
    "    # print(df_filt.columns.values)\n",
    "    # print(df_filt.shape)\n",
    "\n",
    "    # Change the dataframe column names with the names defined in the cell before\n",
    "    df_meas = df_meas.rename(columns=dict(zip(df_meas.columns.values, ['Timestamp'] + measurement_names)))\n",
    "    df_filt_state = df_filt_state.rename(columns=dict(zip(df_filt_state.columns.values, ['Timestamp'] + state_names)))\n",
    "    df_filt_var = df_filt_var.rename(columns=dict(zip(df_filt_var.columns.values, ['Timestamp'] + state_names)))\n",
    "\n",
    "    print(\"df_meas\")\n",
    "    print(df_meas.columns.values)\n",
    "    print(df_meas.shape)\n",
    "    print(\"df_filt_state\")\n",
    "    print(df_filt_state.columns.values)\n",
    "    print(df_filt_state.shape)\n",
    "    print(\"df_filt_var\")\n",
    "    print(df_filt_var.columns.values)\n",
    "    print(df_filt_var.shape)\n",
    "    \n",
    "    # Set the 'Timestamp' column as the index\n",
    "    df_meas = df_meas.set_index('Timestamp')\n",
    "    df_filt_state = df_filt_state.set_index('Timestamp') \n",
    "    df_filt_var = df_filt_var.set_index('Timestamp')\n",
    "\n",
    "    # Store the unpacked dataframes in the final_npz_dfs dictionary\n",
    "    final_npz_dfs[subj] = {'df_meas': df_meas, 'df_filt_state': df_filt_state, 'df_filt_var': df_filt_var}\n",
    "\n",
    "    sys.exit()  \n",
    "    # for column in columns:\n",
    "    #     df = df[column].apply(pd.Series)\n",
    "    \n",
    "    # print(df.shape)\n",
    "    # pivoted_df = df.pivot(index='Timestamp', columns=df.columns.values, values=)\n",
    "\n",
    "    # pivoted_df = df.pivot(index='Timestamp', columns='Variable', values='Value')\n",
    "\n",
    "# # PIVOT THE DATAFRAMES\n",
    "# pivoted_df = pd.pivot_table(df, values='value', index='timestamp', columns='variable')\n",
    "\n",
    "# # Pivot the DataFrame\n",
    "# pivoted_df = df.pivot(index='timestamp', columns='topic', values='message') # use the timestamp as the index\n",
    "# pivoted_df = pivoted_df.reset_index() # reset the index to make the timestamp a column\n",
    "# print(f\"Dataframe columns: {pivoted_df.columns.values}\")\n",
    "# # Convert the 'timestamp' column to a TimeDeltaIndex\n",
    "# pivoted_df['timestamp'] = pd.to_timedelta(pivoted_df['timestamp'], unit='s')\n",
    "\n",
    "# # Resample the DataFrame to a known frequency\n",
    "# dt = 0.01\n",
    "# freq_str = f'{dt}S' # seconds\n",
    "# resampled_df = pivoted_df.resample(freq_str, on='timestamp').mean() # compute the mean of the values in each time bin\n",
    "# resampled_df = resampled_df.reset_index() # reset the index to make the timestamp a column"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
