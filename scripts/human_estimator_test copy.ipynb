{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rospkg\n",
    "import numpy as np\n",
    "import tf\n",
    "\n",
    "# Create a RosPack object\n",
    "rospack = rospkg.RosPack()\n",
    "\n",
    "# Get the path to the package this script is in\n",
    "package_path = rospack.get_path('hri_predict_ros')\n",
    "\n",
    "# Define the path to the plots directory\n",
    "plot_dir = os.path.join(package_path, 'plots')\n",
    "if not os.path.exists(plot_dir):\n",
    "    os.makedirs(plot_dir)\n",
    "\n",
    "# Specify which topics to read from the rosbag file\n",
    "topic_names = ['/offline/zed/zed_node/body_trk/skeletons']\n",
    "\n",
    "n_kpts = 18\n",
    "TF_world_camera = [0.100575, -0.9304, 2.31042, 0.180663, 0.516604, 0.119341, 0.828395]\n",
    "\n",
    "translation_world_camera = np.array(TF_world_camera[0:3])\n",
    "quaternion_world_camera = np.array(TF_world_camera[3:7])\n",
    "\n",
    "# Convert the quaternion to a rotation matrix\n",
    "rotation_matrix_world_camera = tf.transformations.quaternion_matrix(quaternion_world_camera)\n",
    "\n",
    "# Create a translation matrix\n",
    "translation_matrix_world_camera = tf.transformations.translation_matrix(translation_world_camera)\n",
    "\n",
    "# Combine the rotation and translation to get the transformation matrix from the world frame to the camera frame\n",
    "cam_to_world_matrix = tf.transformations.concatenate_matrices(\n",
    "    translation_matrix_world_camera,\n",
    "    rotation_matrix_world_camera\n",
    ")\n",
    "\n",
    "human_meas_names = ['human_kp{}_{}'.format(i, suffix)\n",
    "                    for i in range(n_kpts)\n",
    "                    for suffix in ['x', 'y', 'z']]\n",
    "\n",
    "# Define the frequency of the measurements for resampling\n",
    "f = 20 # Hz\n",
    "meas_dt = 1/f\n",
    "freq_str = f'{meas_dt}S' # seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import measurements from bag file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rosbag\n",
    "\n",
    "# Define the path to the bag directory\n",
    "bag_dir = os.path.join(package_path, 'logs', 'bag')\n",
    "\n",
    "bag_files = os.listdir(bag_dir)\n",
    "bag_files = [os.path.join(bag_dir, bag_file) for bag_file in bag_files]\n",
    "\n",
    "bag_data = {}\n",
    "for bag_file in bag_files:\n",
    "    with rosbag.Bag(bag_file, 'r') as bag:\n",
    "        rows_list = []\n",
    "        for topic, msg, t in bag.read_messages(topics=topic_names):\n",
    "            row_dict = {}\n",
    "\n",
    "            timestamp = t.to_sec()\n",
    "\n",
    "            human_meas = np.full((1, n_kpts*3), np.nan)\n",
    "            if topic == '/offline/zed/zed_node/body_trk/skeletons':\n",
    "                skeleton_kpts = np.full((n_kpts, 3), np.nan)\n",
    "                if msg.objects:                \n",
    "                    for obj in msg.objects:\n",
    "                        # Extract skeleton keypoints from message ([x, y, z] for each kpt)\n",
    "                        kpts = np.array([[kp.kp] for kp in obj.skeleton_3d.keypoints])\n",
    "                        kpts = kpts[:n_kpts] # select only the first n_kpts\n",
    "\n",
    "                        skeleton_kpts = np.reshape(kpts, (n_kpts, 3)) # reshape to (n_kpts, 3)\n",
    "\n",
    "                        # Convert keypoints to world frame\n",
    "                        for i in range(n_kpts):\n",
    "                            # Create a homogeneous coordinate for the keypoint position\n",
    "                            kpt = np.array([skeleton_kpts[i][0],\n",
    "                                            skeleton_kpts[i][1],\n",
    "                                            skeleton_kpts[i][2],\n",
    "                                            1])\n",
    "\n",
    "                            # Transform the keypoint to the world frame using the transformation matrix\n",
    "                            kpt_world = np.dot(cam_to_world_matrix, kpt)\n",
    "\n",
    "                            skeleton_kpts[i][0] = kpt_world[0]\n",
    "                            skeleton_kpts[i][1] = kpt_world[1]\n",
    "                            skeleton_kpts[i][2] = kpt_world[2]\n",
    "                    \n",
    "                else:\n",
    "                    skeleton_kpts = np.full(skeleton_kpts.shape, np.nan)\n",
    "\n",
    "                # Update current human measurement vector\n",
    "                human_meas = skeleton_kpts.flatten()\n",
    "\n",
    "            row_dict.update({'timestamp': timestamp})\n",
    "            row_dict.update({'human_meas': human_meas.flatten()})\n",
    "\n",
    "            rows_list.append(row_dict)\n",
    "\n",
    "    subject_id = bag_file.split('.')[0].split('simple_')[-1]\n",
    "    bag_data[subject_id] = rows_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store measurement data in a dictionary of Pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "measurement_data = {}\n",
    "for subject, bag in bag_data.items():\n",
    "    data = pd.DataFrame(bag, columns=['timestamp', 'human_meas'])\n",
    "\n",
    "    # split columns into separate columns\n",
    "    for c in data.columns.values:\n",
    "        data = pd.concat([data, data.pop(c).apply(pd.Series).add_prefix(c+\"_\")], axis=1)\n",
    "\n",
    "    # change column names\n",
    "    data.columns = ['timestamp'] + human_meas_names\n",
    "\n",
    "    # Make time index relative to the start of the recording\n",
    "    # data['timestamp'] = data['timestamp'] - data['timestamp'][0]\n",
    "\n",
    "    # Convert the 'timestamp' column to a TimeDeltaIndex\n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'], unit='s')\n",
    "\n",
    "    # Increase the timestamp by 2 hours to match the system time\n",
    "    data['timestamp'] = data['timestamp'] + pd.Timedelta(hours=2)\n",
    "\n",
    "    # Resample the DataFrame to a known frequency\n",
    "    # resampled_data = data.resample(freq_str, on='timestamp').mean()\n",
    "    #resampled_data = data.resample(freq_str, on='timestamp').first()\n",
    "    measurement_data[subject] = data\n",
    "\n",
    "for key, value in measurement_data.items():\n",
    "    print(f'{key}: \\t\\t{value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "example_data = measurement_data['sub_7']\n",
    "\n",
    "fig = px.scatter(example_data, x=example_data.index, y=['human_kp4_y'])\n",
    "\n",
    "fig.update_traces(marker=dict(size=3))\n",
    "fig.update_layout(title='kp4_y', xaxis_title='Timestamp', yaxis_title='Value')\n",
    "fig.update_xaxes(tickformat='%H:%M:%S')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import task data from GUI logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gui_dir = os.path.join(package_path, 'logs', 'gui_data')\n",
    "\n",
    "# Get all the file names in the directory\n",
    "gui_files = os.listdir(gui_dir)\n",
    "\n",
    "gui_data = {}\n",
    "for gui_file in gui_files:\n",
    "    # Check if the file is a text file\n",
    "    if gui_file.endswith('.txt'): # they are txt files, but structured as csv\n",
    "        # Construct the file path\n",
    "        file_path = os.path.join(gui_dir, gui_file)\n",
    "        \n",
    "        # Read the file as a dataframe\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Add the dataframe to the dictionary using a portion of the file name as the key\n",
    "        key = gui_file.split('.')[0].split('_')[-2:]\n",
    "        key = '_'.join(key)\n",
    "        gui_data[key] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define ranges from gui data to split measurements in tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VELOCITIES = ['SLOW', 'MEDIUM', 'FAST']\n",
    "TASK_NAMES = ['PICK-&-PLACE', 'WALKING', 'PASSING-BY']\n",
    "\n",
    "trigger_data = {}\n",
    "for subject, gui in gui_data.items():\n",
    "    for velocity in VELOCITIES:\n",
    "        for task in TASK_NAMES:\n",
    "            trigger_data[(subject, velocity, task)] = gui.loc[(gui['Velocity'] == velocity) & (gui['Task_name'] == task)]\n",
    "\n",
    "            # only keep the first and last timestamp of each trigger_data\n",
    "            first_last_timestamps = trigger_data[(subject, velocity, task)].iloc[[0, -1]]['Timestamp'].values\n",
    "            trigger_data[(subject, velocity, task)] = first_last_timestamps\n",
    "\n",
    "for key, value in trigger_data.items():\n",
    "    print(f'{key}: \\t\\t{value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define helper functions for Kalman Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_near_psd(P, max_iter=10):\n",
    "\n",
    "    eps = 1e-3  # Small positive jitter for regularization\n",
    "    increment_factor = 10  # Factor to increase eps if needed\n",
    "        \n",
    "    def is_symmetric(A):\n",
    "        return np.allclose(A, A.T)\n",
    "                    \n",
    "    def is_positive_definite(A):\n",
    "        try:\n",
    "            np.linalg.cholesky(A)\n",
    "            return True\n",
    "        except np.linalg.LinAlgError:\n",
    "            return False\n",
    "        \n",
    "    for _ in range(max_iter):\n",
    "        if is_symmetric(P) and is_positive_definite(P):\n",
    "            return P  # The matrix is suitable for Cholesky\n",
    "    \n",
    "        # Make P symmetric\n",
    "        P = (P + P.T) / 2\n",
    "    \n",
    "        # Set negative eigenvalues to zero\n",
    "        eigval, eigvec = np.linalg.eig(P)\n",
    "        eigval[eigval < 0] = 0\n",
    "        # add a jitter for strictly positive\n",
    "        eigval += eps\n",
    "    \n",
    "        # Reconstruct the matrix\n",
    "        P = eigvec.dot(np.diag(eigval)).dot(eigvec.T)\n",
    "\n",
    "        # Force P to be real\n",
    "        P = np.real(P)\n",
    "\n",
    "        # Force off-diagonal elements to be zero => do it since the keypoints are all independent\n",
    "        # P = np.diag(np.diag(P))\n",
    "    \n",
    "        # Check if P is now positive definite\n",
    "        if is_positive_definite(P):\n",
    "            return P\n",
    "    \n",
    "        # Increase regularization factor for the next iteration\n",
    "        eps *= increment_factor\n",
    "    \n",
    "    raise ValueError(\"Unable to convert the matrix to positive definite within max iterations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define parameters for the filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from filterpy.kalman import UnscentedKalmanFilter, MerweScaledSigmaPoints\n",
    "from filterpy.kalman import IMMEstimator\n",
    "from filterpy.common import Q_discrete_white_noise\n",
    "from scipy.linalg import block_diag\n",
    "import copy, time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Parameters\n",
    "dt = 0.1\n",
    "predict_k_steps = True\n",
    "pred_steps = 5          # [paper: n] number of prediction steps\n",
    "n_kpts = 18\n",
    "n_var_per_dof = 3       # position, velocity, acceleration\n",
    "n_dim_per_kpt = 3       # x, y, z\n",
    "var_r = 0.0025          # [paper: r_y] Hip: 3-sigma (99.5%) = 0.15 m ==> sigma 0.05 m ==> var = (0.05)^2 m^2\n",
    "var_q = 0.1             # [paper: q_a] a_dot = u (u = 0 is very uncertain ==> add variance here)\n",
    "var_P_pos = var_r       # [paper: p_y] Set equal to the measurement noise since the state is initialized with the measurement\n",
    "var_P_vel = 0.02844     # [paper: p_v] Hip: no keypoint moves faster than 1.6 m/s ==> 3-sigma (99.5%) = 1.6 m/s ==> var = (1.6/3)^2 m^2/s^2\n",
    "var_P_acc = 1.1111      # [paper: p_a] Hip: no keypoint accelerates faster than 10 m/s^2 ==> 3-sigma (99.5%) = 10 m/s^2 ==> var = (10/3)^2 m^2/s^4\n",
    "max_time_no_meas = pd.Timedelta(seconds=1.0)\n",
    "\n",
    "# Transition matrix for IMM\n",
    "NUM_FILTERS_IN_BANK = 3\n",
    "M = np.array([[0.55, 0.15, 0.30],\n",
    "              [0.15, 0.75, 0.10],\n",
    "              [0.60, 0.30, 0.10]])\n",
    "mu = np.array([0.55, 0.40, 0.05])\n",
    "\n",
    "# Dimensions\n",
    "dim_x = n_var_per_dof * n_dim_per_kpt * n_kpts # 3D (position, velocity, acceleration) for each keypoint\n",
    "dim_z = n_dim_per_kpt * n_kpts # 3D position for each keypoint\n",
    "\n",
    "# Initial covariance matrix for all keypoints\n",
    "def initialize_P(n_dim_per_kpt, n_kpts, var_P_pos, var_P_vel, var_P_acc):\n",
    "    if n_var_per_dof == 1:\n",
    "        init_P = np.eye(n_dim_per_kpt * n_kpts) * var_P_pos\n",
    "    elif n_var_per_dof == 2:\n",
    "        init_P = np.diag([var_P_pos, var_P_vel]) # initial state covariance for the single keypoint\n",
    "        init_P = block_diag(*[init_P for _ in range(n_dim_per_kpt * n_kpts)])\n",
    "    elif n_var_per_dof == 3:\n",
    "        init_P = np.diag([var_P_pos, var_P_vel, var_P_acc]) # initial state covariance for the single keypoint\n",
    "        init_P = block_diag(*[init_P for _ in range(n_dim_per_kpt * n_kpts)])\n",
    "    else:\n",
    "        raise ValueError('Invalid n_var_per_dof')\n",
    "    return init_P\n",
    "    \n",
    "init_P = initialize_P(n_dim_per_kpt, n_kpts, var_P_pos, var_P_vel, var_P_acc)\n",
    "\n",
    "# Position indices\n",
    "p_idx = np.arange(0, dim_x, n_var_per_dof)\n",
    "\n",
    "# Column names\n",
    "state_names = ['kp{}_{}'.format(i, suffix)\n",
    "               for i in range(n_kpts)\n",
    "               for suffix in ['x', 'xd', 'xdd', 'y', 'yd', 'ydd', 'z', 'zd', 'zdd']]\n",
    "measurement_names = ['kp{}_{}'.format(i, suffix)\n",
    "                     for i in range(n_kpts)\n",
    "                     for suffix in ['x', 'y', 'z']]\n",
    "filtered_column_names = ['{}_kp{}_{}'.format(filt_type, i, suffix)\n",
    "                         for filt_type in ['ca', 'cv', 'imm']\n",
    "                         for i in range(n_kpts)\n",
    "                         for suffix in ['x', 'xd', 'xdd', 'y', 'yd', 'ydd', 'z', 'zd', 'zdd']]\n",
    "filtered_pred_column_names = ['{}_kp{}_{}'.format(filt_type, i, suffix)\n",
    "                              for filt_type in ['ca', 'imm']\n",
    "                              for i in range(n_kpts)\n",
    "                              for suffix in ['x', 'xd', 'xdd', 'y', 'yd', 'ydd', 'z', 'zd', 'zdd']]\n",
    "col_names_imm = ['imm_pos', 'imm_vel', 'imm_acc']\n",
    "col_names_prob_imm = ['prob_ca', 'prob_ca_no', 'prob_cv']\n",
    "\n",
    "# measurement function: only the position is measured\n",
    "def hx(x):\n",
    "    return x[p_idx]\n",
    "\n",
    "sigmas = MerweScaledSigmaPoints(n=dim_x, alpha=.1, beta=2., kappa=1.)\n",
    "\n",
    "# CONSTANT ACCELERATION UKF\n",
    "F_block_ca = np.array([[1, dt, 0.5*dt**2],\n",
    "                       [0, 1, dt],\n",
    "                       [0, 0, 1]])\n",
    "F_ca = block_diag(*[F_block_ca for _ in range(n_dim_per_kpt * n_kpts)])\n",
    "\n",
    "# state transition function: const acceleration\n",
    "def fx_ca(x, dt):\n",
    "    return np.dot(F_ca, x)\n",
    "\n",
    "# CONSTANT VELOCITY UKF\n",
    "F_block_cv = np.array([[1, dt, 0],\n",
    "                       [0, 1, 0],\n",
    "                       [0, 0, 0]])\n",
    "F_cv = block_diag(*[F_block_cv for _ in range(n_dim_per_kpt * n_kpts)])\n",
    "\n",
    "# state transition function: const velocity\n",
    "def fx_cv(x, dt):\n",
    "    return np.dot(F_cv, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute predict-update-k_step_predict loop:\n",
    "- for each subject\n",
    "- for each velocity\n",
    "- for each task\n",
    "\n",
    "and aggregate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_filtering_loop(subject_ids, velocities, task_names, pred_horizons, init_P, var_r, var_q):\n",
    "    # CONSTANT ACCELERATION UKF\n",
    "    ca_ukf = UnscentedKalmanFilter(dim_x=dim_x, dim_z=dim_z, dt=dt, hx=hx, fx=fx_ca, points=sigmas)\n",
    "    ca_ukf.x = np.nan * np.ones(dim_x)\n",
    "    ca_ukf.P = init_P\n",
    "    ca_ukf.R = np.eye(dim_z)* var_r\n",
    "    ca_ukf.Q = Q_discrete_white_noise(dim=n_var_per_dof, dt=dt, var=var_q, block_size=n_dim_per_kpt * n_kpts)\n",
    "\n",
    "    # CONSTANT ACCELERATION UKF WITH NO PROCESS ERROR\n",
    "    ca_no_ukf = copy.deepcopy(ca_ukf)\n",
    "    ca_no_ukf.Q = np.zeros((dim_x, dim_x))\n",
    "\n",
    "    # CONSTANT VELOCITY UKF\n",
    "    cv_ukf = UnscentedKalmanFilter(dim_x=dim_x, dim_z=dim_z, dt=dt, hx=hx, fx=fx_cv, points=sigmas)\n",
    "    cv_ukf.x = np.nan * np.ones(dim_x)\n",
    "    cv_ukf.P = init_P\n",
    "    cv_ukf.R = np.eye(dim_z)* var_r\n",
    "    cv_ukf.Q = Q_discrete_white_noise(dim=n_var_per_dof, dt=dt, var=var_q, block_size=n_dim_per_kpt * n_kpts)\n",
    "    \n",
    "    # IMM ESTIMATOR\n",
    "    filters = [copy.deepcopy(ca_ukf), ca_no_ukf, copy.deepcopy(cv_ukf)]\n",
    "\n",
    "    bank = IMMEstimator(filters, mu, M)\n",
    "\n",
    "    # K-STEP AHEAD PREDICTION FILTERS (declare k dictionaries to store the time series of predicted states and covariances)\n",
    "    ca_ukf_pred = copy.deepcopy(ca_ukf)\n",
    "    uxs_ca_pred = {}\n",
    "    uxs_ca_pred_cov = {}\n",
    "    bank_pred = copy.deepcopy(bank)\n",
    "    uxs_bank_pred = {}\n",
    "    uxs_bank_pred_cov = {}\n",
    "    probs_bank_pred = {}\n",
    "\n",
    "    # Create dictionary to store results\n",
    "    measurement_split = {}   # dictionary of DataFrames with the measurements split by task\n",
    "    filtering_results = {}   # dictionary of dictionaries with the filtering results\n",
    "    prediction_results = {}  # dictionary of dictionaries with the k-step ahead prediction results\n",
    "\n",
    "    for k in pred_horizons:\n",
    "        for subject_id in subject_ids:\n",
    "            for velocity in velocities:\n",
    "                for task in task_names:\n",
    "                    print(f'Processing {subject_id} - {velocity} - {task} for {k} steps ahead...')\n",
    "\n",
    "                    # Reinitialize the lists to store the filtering results\n",
    "                    uxs_ca, uxs_cv, uxs_bank, probs_bank = [], [], [], []\n",
    "                    uxs_ca_cov, uxs_bank_cov = [], []\n",
    "\n",
    "                    # Get the trigger timestamps for the current task\n",
    "                    trigger_timestamps = trigger_data[(subject_id, velocity, task)]\n",
    "\n",
    "                    # Get only the measurements whose timestamps are within the trigger timestamps\n",
    "                    start_trigger = pd.to_datetime(trigger_timestamps[0])\n",
    "                    end_trigger = pd.to_datetime(trigger_timestamps[1])\n",
    "\n",
    "                    print(\"Selecting measurements from: \", start_trigger, \"to\", end_trigger)\n",
    "\n",
    "                    zs = measurement_data[subject_id].loc[(measurement_data[subject_id]['timestamp'] >= start_trigger) &\n",
    "                                                        (measurement_data[subject_id]['timestamp'] <= end_trigger)]\n",
    "                    #zs.set_index('timestamp', inplace=True)\n",
    "\n",
    "                    # Resample the measurements to a known frequency and subtract initial time\n",
    "                    # zs = zs.resample(freq_str).mean()\n",
    "                    #zs.index = zs.index - zs.index[0]\n",
    "                    zs[\"timestamp\"] = zs[\"timestamp\"] - zs[\"timestamp\"].iloc[0]\n",
    "                    \n",
    "                    measurement_split[(k, subject_id, velocity, task)] = zs\n",
    "\n",
    "                    # Define times\n",
    "                    t = zs[\"timestamp\"].iloc[0]\n",
    "                    t_end = zs[\"timestamp\"].iloc[-1]\n",
    "                    t_incr = pd.Timedelta(seconds=dt)\n",
    "\n",
    "                    print(\"Start time:\", t, \"End time:\", t_end)\n",
    "\n",
    "                    # Initialization flag\n",
    "                    time_no_meas = pd.Timedelta(seconds=0)\n",
    "                    ufk_initialized = False\n",
    "                    filt_timestamps = []\n",
    "                    elapsed_time = 0.0\n",
    "\n",
    "                    # Main loop\n",
    "                    total_iterations = int((t_end - t) / t_incr) + 1\n",
    "                    pbar = tqdm(total=total_iterations)\n",
    "\n",
    "                    # Create dictionaries to store the k-step ahead prediction results\n",
    "                    if predict_k_steps:\n",
    "                        for i in range(k):\n",
    "                            uxs_ca_pred[i] = []\n",
    "                            uxs_bank_pred[i] = []\n",
    "                            probs_bank_pred[i] = []\n",
    "                            uxs_ca_pred_cov[i] = []\n",
    "                            uxs_bank_pred_cov[i] = []\n",
    "\n",
    "                    while t <= t_end:\n",
    "                        tic = time.time()\n",
    "                        filt_timestamps.append(t)\n",
    "                        k_step_pred_executed = False\n",
    "\n",
    "                        # Get the measurements in the current time window\n",
    "                        tmp_db =zs.loc[(zs[\"timestamp\"]>=t) & (zs[\"timestamp\"]<=t+t_incr)]\n",
    "                        measure_received = False\n",
    "                        if (tmp_db.shape[0] > 0):\n",
    "                            z = np.double(np.array(tmp_db.iloc[-1][1:])) # Select the last measurement in the time window\n",
    "                            measure_received = not np.isnan(z).any() # Consider the measurement only if it is not NaN\n",
    "                            \n",
    "                        if measure_received and not ufk_initialized:\n",
    "                            # print('timestamp:', t, 'measure:', z, 'initializing filters')\n",
    "                            # initial state: [pos, vel, acc] = [current measured position, 0.0, 0.0]\n",
    "                            ca_ukf.x = np.zeros(dim_x)\n",
    "                            ca_ukf.x[p_idx] = z\n",
    "                            cv_ukf.x = np.zeros(dim_x)\n",
    "                            cv_ukf.x[p_idx] = z\n",
    "                            for f in bank.filters:\n",
    "                                f.x = np.zeros(dim_x)\n",
    "                                f.x[p_idx] = z\n",
    "                            ufk_initialized = True\n",
    "\n",
    "                        else:\n",
    "                            if not measure_received and ufk_initialized:\n",
    "                                time_no_meas += t_incr\n",
    "                                # print('timestamp:', t, 'no measure received for', time_no_meas, 'seconds')\n",
    "\n",
    "                            if time_no_meas >= max_time_no_meas:\n",
    "                                ufk_initialized = False\n",
    "                            \n",
    "                                # Reset filter states\n",
    "                                ca_ukf.x = np.nan * np.ones(dim_x)\n",
    "                                cv_ukf.x = np.nan * np.ones(dim_x)\n",
    "                                bank.x = np.nan * np.ones(dim_x)\n",
    "                                if predict_k_steps:\n",
    "                                    ca_ukf_pred.x = np.nan * np.ones(dim_x)\n",
    "                                    bank_pred.x = np.nan * np.ones(dim_x)\n",
    "\n",
    "                                # Reset filter covariances\n",
    "                                ca_ukf.P = init_P\n",
    "                                cv_ukf.P = init_P\n",
    "                                bank.P = init_P\n",
    "                                if predict_k_steps:\n",
    "                                    ca_ukf_pred.P = init_P\n",
    "                                    bank_pred.P = init_P\n",
    "                                \n",
    "                            if ufk_initialized:\n",
    "                                try:\n",
    "                                    # make sure covariance matrices are positive semidefinite\n",
    "                                    ca_ukf.P = get_near_psd(ca_ukf.P)\n",
    "                                    cv_ukf.P = get_near_psd(cv_ukf.P)\n",
    "                                    for f in bank.filters:\n",
    "                                        f.P = get_near_psd(f.P)\n",
    "                                    \n",
    "                                    ca_ukf.predict()\n",
    "                                    cv_ukf.predict()\n",
    "                                    bank.predict()\n",
    "\n",
    "                                    if measure_received:\n",
    "                                        time_no_meas = pd.Timedelta(seconds=0)\n",
    "                                        ca_ukf.update(z)\n",
    "                                        cv_ukf.update(z)\n",
    "                                        bank.update(z)\n",
    "\n",
    "                                    if predict_k_steps:\n",
    "                                        # Predict k steps ahead starting from the current state and covariance\n",
    "                                        ca_ukf_pred.x = ca_ukf.x.copy()\n",
    "                                        ca_ukf_pred.P = ca_ukf.P.copy()\n",
    "                                        bank_pred.x = bank.x.copy()\n",
    "                                        for f_pred, f in zip(bank_pred.filters, bank.filters):\n",
    "                                            f_pred.x = f.x.copy()\n",
    "                                            f_pred.P = f.P.copy()\n",
    "                                            \n",
    "                                        for i in range(k):\n",
    "                                            # make sure covariance matrices are positive semidefinite\n",
    "                                            ca_ukf_pred.P = get_near_psd(ca_ukf_pred.P)\n",
    "                                            for f in bank_pred.filters:\n",
    "                                                f.P = get_near_psd(f.P)\n",
    "\n",
    "                                            ca_ukf_pred.predict()\n",
    "                                            bank_pred.predict()\n",
    "\n",
    "                                            uxs_ca_pred[i].append(ca_ukf_pred.x.copy())\n",
    "                                            uxs_bank_pred[i].append(bank_pred.x.copy())\n",
    "                                            probs_bank_pred[i].append(bank_pred.mu.copy())\n",
    "                                            uxs_ca_pred_cov[i].append(ca_ukf_pred.P.copy().flatten())\n",
    "                                            uxs_bank_pred_cov[i].append(bank_pred.P.copy().flatten())\n",
    "\n",
    "                                        k_step_pred_executed = True\n",
    "\n",
    "                                except np.linalg.LinAlgError as e:\n",
    "                                    print(f\"LinAlgError: {e}\")\n",
    "\n",
    "                                    # Reset filters\n",
    "                                    ufk_initialized = False\n",
    "                                        \n",
    "                                    # Reset filter states\n",
    "                                    ca_ukf.x = np.nan * np.ones(dim_x)\n",
    "                                    cv_ukf.x = np.nan * np.ones(dim_x)\n",
    "                                    bank.x = np.nan * np.ones(dim_x)\n",
    "                                    if predict_k_steps:\n",
    "                                        ca_ukf_pred.x = np.nan * np.ones(dim_x)\n",
    "                                        bank_pred.x = np.nan * np.ones(dim_x)\n",
    "                                        bank_pred.mu = np.nan * np.ones(NUM_FILTERS_IN_BANK) # IMM probabilities (3 filters)\n",
    "\n",
    "                                    # Reset filter covariances\n",
    "                                    ca_ukf.P = init_P\n",
    "                                    cv_ukf.P = init_P\n",
    "                                    bank.P = init_P\n",
    "                                    if predict_k_steps:\n",
    "                                        ca_ukf_pred.P = init_P\n",
    "                                        bank_pred.P = init_P\n",
    "                                \n",
    "                        uxs_ca.append(ca_ukf.x.copy())\n",
    "                        uxs_cv.append(cv_ukf.x.copy())\n",
    "                        uxs_bank.append(bank.x.copy())\n",
    "                        probs_bank.append(bank.mu.copy())\n",
    "                        uxs_ca_cov.append(ca_ukf.P.copy().flatten())\n",
    "                        uxs_bank_cov.append(bank.P.copy().flatten())\n",
    "\n",
    "                        if not k_step_pred_executed:\n",
    "                            for i in range(k):\n",
    "                                uxs_ca_pred[i].append(ca_ukf.x.copy())\n",
    "                                uxs_bank_pred[i].append(bank.x.copy())\n",
    "                                probs_bank_pred[i].append(bank.mu.copy())\n",
    "                                uxs_ca_pred_cov[i].append(ca_ukf.P.copy().flatten())\n",
    "                                uxs_bank_pred_cov[i].append(bank.P.copy().flatten())\n",
    "\n",
    "                        t += t_incr\n",
    "                        toc = time.time()\n",
    "                        elapsed_time += (toc - tic)\n",
    "                        \n",
    "                        pbar.update()\n",
    "\n",
    "                    pbar.close()\n",
    "                    print(\"Mean loop frequency: {:.2f} Hz\".format(1.0 / (elapsed_time / len(filt_timestamps))))\n",
    "\n",
    "                    # Create DataFrames with the filtered data\n",
    "                    uxs_ca = np.array(uxs_ca)\n",
    "                    uxs_cv = np.array(uxs_cv)\n",
    "                    uxs_bank = np.array(uxs_bank)\n",
    "                    uxs = np.concatenate((uxs_ca, uxs_cv, uxs_bank), axis=1)\n",
    "                    probs_bank = np.array(probs_bank)\n",
    "                    uxs_ca_cov = np.array(uxs_ca_cov)\n",
    "                    uxs_bank_cov = np.array(uxs_bank_cov)\n",
    "                    uxs_cov = np.concatenate((uxs_ca_cov, uxs_bank_cov), axis=1)\n",
    "\n",
    "                    filtered_data = pd.DataFrame(uxs, index=filt_timestamps, columns=filtered_column_names)\n",
    "                    imm_probs = pd.DataFrame(probs_bank, index=filt_timestamps, columns=col_names_prob_imm)\n",
    "                    filtered_data_cov = pd.DataFrame(uxs_cov, index=filt_timestamps) # the elements of the flattened covariance matrices are stored in separate anonymous columns\n",
    "\n",
    "                    if predict_k_steps:\n",
    "                        kstep_pred_data = {}\n",
    "                        kstep_pred_imm_probs = {}\n",
    "                        kstep_pred_cov = {}\n",
    "\n",
    "                        for i in range(k):\n",
    "                            uxs_pred = np.concatenate((np.array(uxs_ca_pred[i]), np.array(uxs_bank_pred[i])), axis=1)\n",
    "                            uxs_pred_cov = np.concatenate((np.array(uxs_ca_pred_cov[i]), np.array(uxs_bank_pred_cov[i])), axis=1)\n",
    "\n",
    "                            kstep_pred_data[i] = pd.DataFrame(uxs_pred, index=filt_timestamps, columns=filtered_pred_column_names)\n",
    "                            kstep_pred_imm_probs[i] = pd.DataFrame(np.array(probs_bank_pred[i]), index=filt_timestamps, columns=col_names_prob_imm)\n",
    "                            kstep_pred_cov[i] = pd.DataFrame(uxs_pred_cov, index=filt_timestamps) # the elements of the flattened covariance matrices are stored in separate anonymous columns\n",
    "\n",
    "                            # Shift the i-step ahead prediction data by i steps\n",
    "                            kstep_pred_data[i] = kstep_pred_data[i].shift(+i)\n",
    "                            kstep_pred_imm_probs[i] = kstep_pred_imm_probs[i].shift(+i)\n",
    "                            kstep_pred_cov[i] = kstep_pred_cov[i].shift(+i)  \n",
    "\n",
    "                    # Store filtering results\n",
    "                    filtering_results[(k, subject_id, velocity, task)] = {\n",
    "                        'filtered_data': filtered_data,\n",
    "                        'imm_probs': imm_probs,\n",
    "                        'filtered_data_cov': filtered_data_cov\n",
    "                    }\n",
    "\n",
    "                    # Store k-step prediction results\n",
    "                    if predict_k_steps: \n",
    "                        prediction_results[(k, subject_id, velocity, task)] = {\n",
    "                            'kstep_pred_data': kstep_pred_data,\n",
    "                            'kstep_pred_imm_probs': kstep_pred_imm_probs,\n",
    "                            'kstep_pred_cov': kstep_pred_cov\n",
    "                        }\n",
    "\n",
    "                    print(f\"Processed {subject_id} - {velocity} - {task} for {k} steps ahead.\")\n",
    "\n",
    "    return measurement_split, filtering_results, prediction_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_horizons = [pred_steps]\n",
    "# measurement_split, filtering_results, prediction_results = run_filtering_loop(['sub_8'], ['FAST'], ['PICK-&-PLACE'], pred_horizons,\n",
    "#                                                                               init_P, var_r, var_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_PLOTLY_COLORS=['rgb(31, 119, 180)', 'rgb(255, 127, 14)',\n",
    "                       'rgb(44, 160, 44)', 'rgb(214, 39, 40)',\n",
    "                       'rgb(148, 103, 189)', 'rgb(140, 86, 75)',\n",
    "                       'rgb(227, 119, 194)', 'rgb(127, 127, 127)',\n",
    "                       'rgb(188, 189, 34)', 'rgb(23, 190, 207)']\n",
    "\n",
    "# Add transparency to the colors\n",
    "alpha = 0.2\n",
    "DEFAULT_PLOTLY_COLORS_ALPHA = [color.replace('rgb', 'rgba').replace(')', f', {alpha})') for color in DEFAULT_PLOTLY_COLORS]\n",
    "\n",
    "def plot_time_series(subject, velocity, task, kpt, dim, description, dim_type, k, n_var_per_dof, n_dim_per_kpt):\n",
    "    state = 'kp{}_{}'.format(kpt, dim)\n",
    "    state_idx = ['x', 'xd', 'xdd', 'y', 'yd', 'ydd', 'z', 'zd', 'zdd'].index(dim) + n_var_per_dof * n_dim_per_kpt * kpt\n",
    "    variance_idx = dim_x * state_idx + state_idx\n",
    "\n",
    "    if dim_type == 'pos':\n",
    "        meas = measurement_split[(k, subject, velocity, task)].copy()\n",
    "        meas_seconds = (meas[\"timestamp\"] - meas[\"timestamp\"].iloc[0]).dt.total_seconds()\n",
    "\n",
    "    filt = filtering_results[(k, subject, velocity, task)]['filtered_data']\n",
    "    filt_seconds = (filt.index - filt.index[0]).total_seconds()\n",
    "\n",
    "    # Display the k-step ahead prediction (only last step)\n",
    "    if predict_k_steps:\n",
    "        kpred = prediction_results[(k, subject, velocity, task)]['kstep_pred_data'][k-1]\n",
    "        kpred_seconds = (kpred.index - kpred.index[0]).total_seconds()\n",
    "        kpred_variance = prediction_results[(k, subject, velocity, task)]['kstep_pred_cov'][k-1]\n",
    "\n",
    "        # CA K-step ahead prediction\n",
    "        std = kpred_variance.iloc[:, variance_idx].apply(np.sqrt)\n",
    "\n",
    "        # Create pandas Series for the upper and lower confidence limits (1-sigma)\n",
    "        kpred['_'.join(['ca', state, 'ucl'])] = kpred['_'.join(['ca', state])] + 1 * std\n",
    "        kpred['_'.join(['ca', state, 'lcl'])] = kpred['_'.join(['ca', state])] - 1 * std\n",
    "\n",
    "        # IMM K-step ahead prediction\n",
    "        # (the index must be increased by the dimension of the flattened covariance matrix according to the way covariance matrices are stored)\n",
    "        std = kpred_variance.iloc[:, variance_idx + dim_x*dim_x].apply(np.sqrt)\n",
    "\n",
    "        # Create pandas Series for the upper and lower confidence limits (1-sigma)\n",
    "        kpred['_'.join(['imm', state, 'ucl'])] = kpred['_'.join(['imm', state])] + 1 * std\n",
    "        kpred['_'.join(['imm', state, 'lcl'])] = kpred['_'.join(['imm', state])] - 1 * std\n",
    "\n",
    "    fig = px.line()\n",
    "    if dim_type == 'pos':\n",
    "        fig.add_scatter(x=meas_seconds,\n",
    "                        y=meas['_'.join(('human',state))],\n",
    "                        mode='lines+markers',\n",
    "                        name='Measurements',\n",
    "                        line=dict(color=DEFAULT_PLOTLY_COLORS[0])\n",
    "        )\n",
    "    fig.add_scatter(x=filt_seconds,\n",
    "                    y=filt['_'.join(('ca',state))],\n",
    "                    mode='lines+markers',\n",
    "                    name='UKF CA',\n",
    "                    line=dict(color=DEFAULT_PLOTLY_COLORS[1])\n",
    "    )\n",
    "    fig.add_scatter(x=filt_seconds,\n",
    "                    y=filt['_'.join(('cv',state))],\n",
    "                    mode='lines+markers',\n",
    "                    name='UKF CV',\n",
    "                    line=dict(color=DEFAULT_PLOTLY_COLORS[5])\n",
    "    )\n",
    "    fig.add_scatter(x=filt_seconds,\n",
    "                    y=filt['_'.join(('imm',state))],\n",
    "                    mode='lines+markers',\n",
    "                    name='UKF IMM',\n",
    "                    line=dict(color=DEFAULT_PLOTLY_COLORS[3])\n",
    "    )\n",
    "    if predict_k_steps:\n",
    "        fig.add_scatter(x=kpred_seconds,\n",
    "                        y=kpred['_'.join(('ca',state))],\n",
    "                        mode='lines+markers',\n",
    "                        name=f'CA {k}-step ahead prediction',\n",
    "                        line=dict(color=DEFAULT_PLOTLY_COLORS[4])\n",
    "        )\n",
    "        fig.add_scatter(x=kpred_seconds,\n",
    "                        y=kpred['_'.join(('imm',state))],\n",
    "                        mode='lines+markers',\n",
    "                        name=f'IMM {k}-step ahead prediction',\n",
    "                        line=dict(color=DEFAULT_PLOTLY_COLORS[2])\n",
    "        )\n",
    "        fig.add_scatter(x=kpred_seconds,\n",
    "                        y=kpred['_'.join(['ca', state, 'ucl'])],\n",
    "                        mode='lines',\n",
    "                        name=f'CA {k}-step ahead prediction (UCL)',\n",
    "                        marker=dict(color=DEFAULT_PLOTLY_COLORS_ALPHA[4]),\n",
    "                        line=dict(width=0),\n",
    "                        fillcolor=DEFAULT_PLOTLY_COLORS_ALPHA[4],\n",
    "        )\n",
    "        fig.add_scatter(x=kpred_seconds,\n",
    "                        y=kpred['_'.join(['ca', state, 'lcl'])],\n",
    "                        mode='lines',\n",
    "                        name=f'CA {k}-step ahead prediction (LCL)',\n",
    "                        marker=dict(color=DEFAULT_PLOTLY_COLORS_ALPHA[4]),\n",
    "                        line=dict(width=0),\n",
    "                        fillcolor=DEFAULT_PLOTLY_COLORS_ALPHA[4],\n",
    "                        fill='tonexty'\n",
    "        )\n",
    "        fig.add_scatter(x=kpred_seconds,\n",
    "                        y=kpred['_'.join(['imm', state, 'ucl'])],\n",
    "                        mode='lines',\n",
    "                        name=f'IMM {k}-step ahead prediction (UCL)',\n",
    "                        marker=dict(color=DEFAULT_PLOTLY_COLORS_ALPHA[2]),\n",
    "                        line=dict(width=0),\n",
    "                        fillcolor=DEFAULT_PLOTLY_COLORS_ALPHA[2]\n",
    "        )\n",
    "        fig.add_scatter(x=kpred_seconds,\n",
    "                        y=kpred['_'.join(['imm', state, 'lcl'])],\n",
    "                        mode='lines',\n",
    "                        name=f'IMM {k}-step ahead prediction (LCL)',\n",
    "                        marker=dict(color=DEFAULT_PLOTLY_COLORS_ALPHA[2]),\n",
    "                        line=dict(width=0),\n",
    "                        fillcolor=DEFAULT_PLOTLY_COLORS_ALPHA[2],\n",
    "                        fill='tonexty'\n",
    "        )\n",
    "\n",
    "    fig.update_traces(marker=dict(size=2), line=dict(width=1))\n",
    "\n",
    "    if dim_type == 'pos':\n",
    "        fig.update_layout(title=description+\" position\"+f\" [{subject}, {velocity}, {task}] \"+f\" (k={k})\",\n",
    "                          xaxis_title='Time (s)',\n",
    "                          yaxis_title='Position (m)',\n",
    "                          hovermode=\"x\")\n",
    "    elif dim_type == 'vel':\n",
    "        fig.update_layout(title=description+\" velocity\"+f\" [{subject}, {velocity}, {task}] \"+f\" (k={k})\",\n",
    "                          xaxis_title='Time (s)',\n",
    "                          yaxis_title='Velocity (m/s)',\n",
    "                          hovermode=\"x\")\n",
    "    elif dim_type == 'acc':\n",
    "        fig.update_layout(title=description+\" acceleration\"+f\" [{subject}, {velocity}, {task}] \"+f\" (dt={dt}, k={k})\",\n",
    "                          xaxis_title='Time (s)',\n",
    "                          yaxis_title='Acceleration (m/s^2)',\n",
    "                          hovermode=\"x\")\n",
    "    else:\n",
    "        raise ValueError(\"Invalid dimension type. Use 'pos', 'vel', or 'acc'.\")\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # Save the plot to the plots folder in html format\n",
    "    plot_name = '_'.join([subject, velocity, task, state, \"dt\", str(dt), \"k\", str(k), str(dim_type), \"pred\", str(predict_k_steps)])\n",
    "    fig.write_html(os.path.join(plot_dir, plot_name+'.html')) # interactive plot\n",
    "    fig.write_image(os.path.join(plot_dir, plot_name+ '.pdf')) # static plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject = 'sub_8'\n",
    "# velocity = 'FAST'\n",
    "# task = 'PICK-&-PLACE'\n",
    "# pred_horizon = 5\n",
    "# kpt = 2\n",
    "# dim = 'x' \n",
    "# dim_type = 'pos' # ['pos', 'vel', 'acc'] # WORKING ?\n",
    "# description = 'Right-hand wrist keypoint x-coordinate'\n",
    "# # description = \"Neck keypoint y-coordinate\"\n",
    "\n",
    "# plot_time_series(subject, velocity, task, kpt, dim, description, dim_type, pred_horizon, n_var_per_dof, n_dim_per_kpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_covariance_cone(subject, velocity, task, kpt, dim, description, dim_type, k, selected_timestamps, selected_range):\n",
    "    state = 'kp{}_{}'.format(kpt, dim)\n",
    "    state_idx = ['x', 'xd', 'xdd', 'y', 'yd', 'ydd', 'z', 'zd', 'zdd'].index(dim) + n_var_per_dof * n_dim_per_kpt * kpt\n",
    "    variance_idx = dim_x * state_idx + state_idx\n",
    "\n",
    "    if dim_type == 'pos':\n",
    "        meas = measurement_split[(k, subject, velocity, task)].copy()\n",
    "        meas_seconds = (meas[\"timestamp\"] - meas[\"timestamp\"].iloc[0]).dt.total_seconds()\n",
    "\n",
    "    filt = filtering_results[(k, subject, velocity, task)]['filtered_data']\n",
    "    filt_cov = filtering_results[(k, subject, velocity, task)]['filtered_data_cov']\n",
    "    filt_seconds = (filt.index - filt.index[0]).total_seconds()\n",
    "\n",
    "    fig = px.line()\n",
    "\n",
    "    # Display the k-step ahead prediction (only last step)\n",
    "    next_dataframes = {}\n",
    "    for t in selected_timestamps:\n",
    "        # Initialize lists to store the next k-step ahead states and confidence limits (1-sigma)\n",
    "        std = np.sqrt(filt_cov.loc[t].iloc[variance_idx])\n",
    "        next_k_states_ca = [filt.loc[t, '_'.join(['ca', state])]]\n",
    "        next_k_lcls_ca = [filt.loc[t, '_'.join(['ca', state])] - 1 * std]\n",
    "        next_k_ucls_ca = [filt.loc[t, '_'.join(['ca', state])] + 1 * std]\n",
    "        next_k_states_imm = [filt.loc[t, '_'.join(['imm', state])]]\n",
    "        next_k_lcls_imm = [filt.loc[t, '_'.join(['imm', state])] - 1 * std]\n",
    "        next_k_ucls_imm = [filt.loc[t, '_'.join(['imm', state])] + 1 * std]\n",
    "        times = [t]\n",
    "\n",
    "        for step in range(k):\n",
    "            # shift the timestamp by step*dt (had been shifted before for visualization purposes)\n",
    "            t_shifted = t - pd.Timedelta(seconds=(step+1)*dt) \n",
    "\n",
    "            kpred = prediction_results[(k, subject, velocity, task)]['kstep_pred_data'][step]\n",
    "            kpred_variance = prediction_results[(k, subject, velocity, task)]['kstep_pred_cov'][step]\n",
    "\n",
    "            # CA K-step ahead prediction\n",
    "            std = kpred_variance.iloc[:, variance_idx].apply(np.sqrt)\n",
    "\n",
    "            # Create pandas Series for the upper and lower confidence limits (1-sigma)\n",
    "            kpred['_'.join(['ca', state, 'ucl'])] = kpred['_'.join(['ca', state])] + 1 * std\n",
    "            kpred['_'.join(['ca', state, 'lcl'])] = kpred['_'.join(['ca', state])] - 1 * std\n",
    "\n",
    "            # IMM K-step ahead prediction\n",
    "            # (the index must be increased by the dimension of the flattened covariance matrix according to the way covariance matrices are stored)\n",
    "            std = kpred_variance.iloc[:, variance_idx + dim_x*dim_x].apply(np.sqrt)\n",
    "\n",
    "            # Create pandas Series for the upper and lower confidence limits (1-sigma)\n",
    "            kpred['_'.join(['imm', state, 'ucl'])] = kpred['_'.join(['imm', state])] + 1 * std\n",
    "            kpred['_'.join(['imm', state, 'lcl'])] = kpred['_'.join(['imm', state])] - 1 * std\n",
    "\n",
    "            t_shifted = t + pd.Timedelta(seconds=(step+1)*dt)\n",
    "\n",
    "            # Store the next k-step ahead states and confidence limits\n",
    "            next_k_states_ca.append(kpred.loc[t_shifted, '_'.join(['ca', state])])\n",
    "            next_k_lcls_ca.append(kpred.loc[t_shifted, '_'.join(['ca', state, 'lcl'])])\n",
    "            next_k_ucls_ca.append(kpred.loc[t_shifted, '_'.join(['ca', state, 'ucl'])])\n",
    "            next_k_states_imm.append(kpred.loc[t_shifted, '_'.join(['imm', state])])\n",
    "            next_k_lcls_imm.append(kpred.loc[t_shifted, '_'.join(['imm', state, 'lcl'])])\n",
    "            next_k_ucls_imm.append(kpred.loc[t_shifted, '_'.join(['imm', state, 'ucl'])])\n",
    "            \n",
    "            # create array of timestamps from to t + k*dt\n",
    "            times.append(t + pd.Timedelta(seconds=(step+1)*dt))\n",
    "\n",
    "        # Transform the lists into Pandas DataFrames\n",
    "        next_k_states_ca = pd.DataFrame(next_k_states_ca, index=times, columns=['CA'])\n",
    "        next_k_lcls_ca = pd.DataFrame(next_k_lcls_ca, index=times, columns=['CA_LCL'])\n",
    "        next_k_ucls_ca = pd.DataFrame(next_k_ucls_ca, index=times, columns=['CA_UCL'])\n",
    "        next_k_states_imm = pd.DataFrame(next_k_states_imm, index=times, columns=['IMM'])\n",
    "        next_k_lcls_imm = pd.DataFrame(next_k_lcls_imm, index=times, columns=['IMM_LCL'])\n",
    "        next_k_ucls_imm = pd.DataFrame(next_k_ucls_imm, index=times, columns=['IMM_UCL'])\n",
    "\n",
    "        next_dataframes[t] = {\n",
    "            'next_k_states_ca': next_k_states_ca,\n",
    "            'next_k_lcls_ca': next_k_lcls_ca,\n",
    "            'next_k_ucls_ca': next_k_ucls_ca,\n",
    "            'next_k_states_imm': next_k_states_imm,\n",
    "            'next_k_lcls_imm': next_k_lcls_imm,\n",
    "            'next_k_ucls_imm': next_k_ucls_imm\n",
    "        }\n",
    "\n",
    "        next_pred_ca = next_dataframes[t]['next_k_states_ca']['CA']\n",
    "        next_pred_ca_lcl = next_dataframes[t]['next_k_lcls_ca']['CA_LCL']\n",
    "        next_pred_ca_ucl = next_dataframes[t]['next_k_ucls_ca']['CA_UCL']\n",
    "        next_pred_imm = next_dataframes[t]['next_k_states_imm']['IMM']\n",
    "        next_pred_imm_lcl = next_dataframes[t]['next_k_lcls_imm']['IMM_LCL']\n",
    "        next_pred_imm_ucl = next_dataframes[t]['next_k_ucls_imm']['IMM_UCL']\n",
    "\n",
    "        next_pred_seconds = next_dataframes[t]['next_k_states_ca'].index.total_seconds()\n",
    "\n",
    "        # Select time range before plotting between selected_range[0] and selected_range[1]\n",
    "        meas_range = (meas['timestamp'] >= selected_range[0]) & (meas['timestamp'] <= selected_range[1])\n",
    "        meas_cut = meas.loc[meas_range]\n",
    "        filt_cut = filt.loc[selected_range[0]:selected_range[1]]\n",
    "        kpred_cut = kpred.loc[selected_range[0]:selected_range[1]]\n",
    "\n",
    "        filt_seconds = filt.index[(filt.index >= selected_range[0]) & (filt.index <= selected_range[1])].total_seconds()\n",
    "        meas_seconds = meas_seconds[meas_range]\n",
    "        kpred_seconds = filt_seconds\n",
    "\n",
    "        fig.add_scatter(x=next_pred_seconds,\n",
    "                        y=next_pred_ca,\n",
    "                        mode='lines+markers',\n",
    "                        name=f'CA next {k}-step at time {t}',\n",
    "                        line=dict(color=DEFAULT_PLOTLY_COLORS[9]),\n",
    "                        showlegend=False\n",
    "        )\n",
    "        fig.add_scatter(x=next_pred_seconds,\n",
    "                        y=next_pred_imm,\n",
    "                        mode='lines+markers',\n",
    "                        name=f'IMM next {k}-step at time {t}',\n",
    "                        line=dict(color=DEFAULT_PLOTLY_COLORS[7]),\n",
    "                        showlegend=False\n",
    "        )\n",
    "        fig.add_scatter(x=next_pred_seconds,\n",
    "                        y=next_pred_ca_lcl,\n",
    "                        mode='lines',\n",
    "                        name=f'CA next {k}-step at time {t} (LCL)',\n",
    "                        marker=dict(color=DEFAULT_PLOTLY_COLORS_ALPHA[9]),\n",
    "                        line=dict(width=0),\n",
    "                        fillcolor=DEFAULT_PLOTLY_COLORS_ALPHA[9],\n",
    "                        showlegend=False\n",
    "        )\n",
    "        fig.add_scatter(x=next_pred_seconds,\n",
    "                        y=next_pred_ca_ucl,\n",
    "                        mode='lines',\n",
    "                        name=f'CA next {k}-step at time {t} (UCL)',\n",
    "                        marker=dict(color=DEFAULT_PLOTLY_COLORS_ALPHA[9]),\n",
    "                        line=dict(width=0),\n",
    "                        fillcolor=DEFAULT_PLOTLY_COLORS_ALPHA[9],\n",
    "                        fill='tonexty',\n",
    "                        showlegend=False\n",
    "        )\n",
    "        fig.add_scatter(x=next_pred_seconds,\n",
    "                        y=next_pred_imm_lcl,\n",
    "                        mode='lines',\n",
    "                        name=f'IMM next {k}-step at time {t} (LCL)',\n",
    "                        marker=dict(color=DEFAULT_PLOTLY_COLORS_ALPHA[7]),\n",
    "                        line=dict(width=0),\n",
    "                        fillcolor=DEFAULT_PLOTLY_COLORS_ALPHA[7],\n",
    "                        showlegend=False\n",
    "        )\n",
    "        fig.add_scatter(x=next_pred_seconds,\n",
    "                        y=next_pred_imm_ucl,\n",
    "                        mode='lines',\n",
    "                        name=f'IMM next {k}-step at time {t} (UCL)',\n",
    "                        marker=dict(color=DEFAULT_PLOTLY_COLORS_ALPHA[7]),\n",
    "                        line=dict(width=0),\n",
    "                        fillcolor=DEFAULT_PLOTLY_COLORS_ALPHA[7],\n",
    "                        fill='tonexty',\n",
    "                        showlegend=False\n",
    "        )\n",
    "\n",
    "\n",
    "    if dim_type == 'pos':\n",
    "        fig.add_scatter(x=meas_seconds,\n",
    "                        y=meas_cut['_'.join(('human',state))],\n",
    "                        mode='lines+markers',\n",
    "                        name='Measurements',\n",
    "                        line=dict(color=DEFAULT_PLOTLY_COLORS[0])\n",
    "        )\n",
    "    fig.add_scatter(x=filt_seconds,\n",
    "                    y=filt_cut['_'.join(('ca',state))],\n",
    "                    mode='lines+markers',\n",
    "                    name='UKF CA',\n",
    "                    line=dict(color=DEFAULT_PLOTLY_COLORS[1])\n",
    "    )\n",
    "    fig.add_scatter(x=filt_seconds,\n",
    "                    y=filt_cut['_'.join(('cv',state))],\n",
    "                    mode='lines+markers',\n",
    "                    name='UKF CV',\n",
    "                    line=dict(color=DEFAULT_PLOTLY_COLORS[5])\n",
    "    )\n",
    "    fig.add_scatter(x=filt_seconds,\n",
    "                    y=filt_cut['_'.join(('imm',state))],\n",
    "                    mode='lines+markers',\n",
    "                    name='UKF IMM',\n",
    "                    line=dict(color=DEFAULT_PLOTLY_COLORS[3])\n",
    "    )\n",
    "    if predict_k_steps:\n",
    "        fig.add_scatter(x=kpred_seconds,\n",
    "                        y=kpred_cut['_'.join(('ca',state))],\n",
    "                        mode='lines+markers',\n",
    "                        name=f'CA {k}-step ahead prediction',\n",
    "                        line=dict(color=DEFAULT_PLOTLY_COLORS[4])\n",
    "        )\n",
    "        fig.add_scatter(x=kpred_seconds,\n",
    "                        y=kpred_cut['_'.join(('imm',state))],\n",
    "                        mode='lines+markers',\n",
    "                        name=f'IMM {k}-step ahead prediction',\n",
    "                        line=dict(color=DEFAULT_PLOTLY_COLORS[2])\n",
    "        )\n",
    "        fig.add_scatter(x=kpred_seconds,\n",
    "                        y=kpred_cut['_'.join(['ca', state, 'ucl'])],\n",
    "                        mode='lines',\n",
    "                        name=f'CA {k}-step ahead prediction (UCL)',\n",
    "                        marker=dict(color=DEFAULT_PLOTLY_COLORS_ALPHA[4]),\n",
    "                        line=dict(width=0),\n",
    "                        fillcolor=DEFAULT_PLOTLY_COLORS_ALPHA[4],\n",
    "        )\n",
    "        fig.add_scatter(x=kpred_seconds,\n",
    "                        y=kpred_cut['_'.join(['ca', state, 'lcl'])],\n",
    "                        mode='lines',\n",
    "                        name=f'CA {k}-step ahead prediction (LCL)',\n",
    "                        marker=dict(color=DEFAULT_PLOTLY_COLORS_ALPHA[4]),\n",
    "                        line=dict(width=0),\n",
    "                        fillcolor=DEFAULT_PLOTLY_COLORS_ALPHA[4],\n",
    "                        fill='tonexty'\n",
    "        )\n",
    "        fig.add_scatter(x=kpred_seconds,\n",
    "                        y=kpred_cut['_'.join(['imm', state, 'ucl'])],\n",
    "                        mode='lines',\n",
    "                        name=f'IMM {k}-step ahead prediction (UCL)',\n",
    "                        marker=dict(color=DEFAULT_PLOTLY_COLORS_ALPHA[2]),\n",
    "                        line=dict(width=0),\n",
    "                        fillcolor=DEFAULT_PLOTLY_COLORS_ALPHA[2]\n",
    "        )\n",
    "        fig.add_scatter(x=kpred_seconds,\n",
    "                        y=kpred_cut['_'.join(['imm', state, 'lcl'])],\n",
    "                        mode='lines',\n",
    "                        name=f'IMM {k}-step ahead prediction (LCL)',\n",
    "                        marker=dict(color=DEFAULT_PLOTLY_COLORS_ALPHA[2]),\n",
    "                        line=dict(width=0),\n",
    "                        fillcolor=DEFAULT_PLOTLY_COLORS_ALPHA[2],\n",
    "                        fill='tonexty'\n",
    "        )\n",
    "        # Add two vertical lines, one at the first element of selected_timestamps and another 0.5 seconds after\n",
    "        # (the line must touch the top and the bottom of the plot)\n",
    "        fig.add_vrect(x0=selected_timestamps[0].total_seconds(),\n",
    "                      x1=(selected_timestamps[0] + pd.Timedelta(seconds=0.5)).total_seconds(),\n",
    "                      fillcolor=\"LightSalmon\",\n",
    "                      opacity=0.5,\n",
    "                      layer=\"below\",\n",
    "                      line_width=0\n",
    "        )\n",
    "    # Add the horizontal arrow\n",
    "    fig.add_annotation(\n",
    "        x=((2*selected_timestamps[0] + pd.Timedelta(seconds=0.5)).total_seconds() / 2),  # This will center the arrow\n",
    "        y=1.5,  # This will place the arrow on top of the plot\n",
    "        text=\"\",\n",
    "        showarrow=True,\n",
    "        arrowhead=1,\n",
    "        arrowsize=1,\n",
    "        arrowwidth=2,\n",
    "        arrowcolor=\"#636363\",\n",
    "        ax=-50, # This will make the arrow point to the right\n",
    "        ay=0\n",
    "    )\n",
    "\n",
    "    fig.update_traces(marker=dict(size=2), line=dict(width=1))\n",
    "\n",
    "    if dim_type == 'pos':\n",
    "        fig.update_layout(title=description+\" position\"+f\" [{subject}, {velocity}, {task}] \"+f\" (k={k})\",\n",
    "                        xaxis_title='Time (s)',\n",
    "                        yaxis_title='Position (m)',\n",
    "                        hovermode=\"x\")\n",
    "    elif dim_type == 'vel':\n",
    "        fig.update_layout(title=description+\" velocity\"+f\" [{subject}, {velocity}, {task}] \"+f\" (k={k})\",\n",
    "                        xaxis_title='Time (s)',\n",
    "                        yaxis_title='Velocity (m/s)',\n",
    "                        hovermode=\"x\")\n",
    "    elif dim_type == 'acc':\n",
    "        fig.update_layout(title=description+\" acceleration\"+f\" [{subject}, {velocity}, {task}] \"+f\" (dt={dt}, k={k})\",\n",
    "                        xaxis_title='Time (s)',\n",
    "                        yaxis_title='Acceleration (m/s^2)',\n",
    "                        hovermode=\"x\")\n",
    "    else:\n",
    "        raise ValueError(\"Invalid dimension type. Use 'pos', 'vel', or 'acc'.\")\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # Save the plot to the plots folder in html format\n",
    "    plot_name = '_'.join([subject, velocity, task, state, \"dt\", str(dt), \"k\", str(k), str(dim_type), \"pred\", str(predict_k_steps), \"cone\"])\n",
    "    fig.write_html(os.path.join(plot_dir, plot_name+ '.html')) # interactive plot\n",
    "    fig.write_image(os.path.join(plot_dir, plot_name+ '.pdf')) # static plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the covariance cone estimated at a given interval for k-steps ahead in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # timestamp0 = pd.Timedelta(seconds=3.8)\n",
    "# # timestamp1 = pd.Timedelta(seconds=4.8)\n",
    "# # timestamp2 = pd.Timedelta(seconds=6.5)\n",
    "\n",
    "# # Define the time step to uniformly sample the time range\n",
    "# cone_step = 1\n",
    "\n",
    "# # Get the first and last timestamps of the measurements\n",
    "# start_meas = measurement_split[(pred_horizon, subject, velocity, task)].timestamp.iloc[0].total_seconds()\n",
    "# end_meas = measurement_split[(pred_horizon, subject, velocity, task)].timestamp.iloc[-1].total_seconds()\n",
    "\n",
    "# # Define a list of pd.Timedelta to uniformly sample the time range\n",
    "# uniform_timestamps = [pd.Timedelta(seconds=s) for s in np.arange(start_meas+dt*pred_steps, end_meas-cone_step, cone_step)]\n",
    "\n",
    "# # Define the time range to plot the covariance cones\n",
    "# lower_bound = pd.Timedelta(seconds=start_meas)\n",
    "# upper_bound = pd.Timedelta(seconds=end_meas)\n",
    "\n",
    "# # selected_timestamps = [timestamp0, timestamp1, timestamp2]\n",
    "# selected_timestamps = uniform_timestamps\n",
    "# selected_range = [lower_bound, upper_bound]\n",
    "\n",
    "# plot_covariance_cone(subject, velocity, task, kpt, dim, description, dim_type, pred_horizon, selected_timestamps, selected_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune parameters on training subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject IDS\n",
    "subject_ids = gui_data.keys()\n",
    "print(\"Subjects: \", subject_ids)\n",
    "\n",
    "# Split subjects into train and test\n",
    "train_subjects = ['sub_9', 'sub_4', 'sub_11', 'sub_7', 'sub_8', 'sub_10', 'sub_6']\n",
    "test_subjects = ['sub_13', 'sub_12', 'sub_3']\n",
    "\n",
    "# Define which keypoints to consider for each task\n",
    "keypoints = {'PICK-&-PLACE': [4, 7], # [2, 3, 4, 5, 6, 7],\n",
    "             'WALKING': [0, 1], # [0, 1, 2, 5, 8, 11],\n",
    "             'PASSING-BY': [0, 1]} # [0, 1, 2, 5, 8, 11]}\n",
    "\n",
    "dimensions_per_keypoint = {0: ['y'],\n",
    "                           1: ['y'],\n",
    "                           4: ['x', 'y', 'z'],\n",
    "                           7: ['x', 'y', 'z']}\n",
    "\n",
    "# Parameters\n",
    "dt = 0.1\n",
    "predict_k_steps = True\n",
    "n_kpts = 18\n",
    "n_var_per_dof = 3       # position, velocity, acceleration\n",
    "n_dim_per_kpt = 3       # x, y, z\n",
    "max_time_no_meas = pd.Timedelta(seconds=1.0)\n",
    "NUM_FILTERS_IN_BANK = 3\n",
    "\n",
    "# Initial values for the parameters\n",
    "var_r = 0.0025          # [paper: r_y] Hip: 3-sigma (99.5%) = 0.15 m ==> sigma 0.05 m ==> var = (0.05)^2 m^2\n",
    "var_q = 0.03            # [paper: q_a] a_dot = u (u = 0 is very uncertain ==> add variance here)\n",
    "var_P_pos = var_r       # [paper: p_y] Set equal to the measurement noise since the state is initialized with the measurement\n",
    "var_P_vel = 0.02844     # [paper: p_v] Hip: no keypoint moves faster than 1.6 m/s ==> 3-sigma (99.5%) = 1.6 m/s ==> var = (1.6/3)^2 m^2/s^2\n",
    "var_P_acc = 1.1111      # [paper: p_a] Hip: no keypoint accelerates faster than 10 m/s^2 ==> 3-sigma (99.5%) = 10 m/s^2 ==> var = (10/3)^2 m^2/s^4\n",
    "\n",
    "M = np.array([[0.55, 0.15, 0.30], # transition matrix for the IMM estimator\n",
    "              [0.15, 0.75, 0.10],\n",
    "              [0.60, 0.30, 0.10]])\n",
    "\n",
    "mu = np.array([0.55, 0.40, 0.05]) # initial mode probabilities for the IMM estimator\n",
    "\n",
    "# Tuning parameters\n",
    "iter_P = 1\n",
    "iter_q = 10\n",
    "decrement_factor_q = 0.75\n",
    "decrement_factor_P = 0.5\n",
    "\n",
    "PRED_HORIZONS = [1, 3, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter var_q tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average error between the filtered states and the k-step ahead predictions\n",
    "def compute_mean_error(filt, kpred, ca_states, imm_states):\n",
    "    ca_error = np.mean(filt[ca_states] - kpred[ca_states])\n",
    "    imm_error = np.mean(filt[imm_states] - kpred[imm_states])\n",
    "\n",
    "    return ca_error, imm_error\n",
    "\n",
    "# Compute the root mean squared error between the filtered states and the k-step ahead predictions\n",
    "def compute_rmse_error(filt, kpred, ca_states, imm_states):\n",
    "    ca_error = np.sqrt(np.mean((filt[ca_states] - kpred[ca_states])**2))\n",
    "    imm_error = np.sqrt(np.mean((filt[imm_states] - kpred[imm_states])**2))\n",
    "\n",
    "    return ca_error, imm_error\n",
    "    \n",
    "# Compute the average standard deviation of the error between the filtered states and the k-step ahead predictions \n",
    "def compute_std_error(filt, kpred, ca_states, imm_states):\n",
    "    ca_error = filt[ca_states] - kpred[ca_states]\n",
    "    ca_error_mean = np.mean(ca_error)\n",
    "    ca_error_std = np.std(ca_error - ca_error_mean)\n",
    "\n",
    "    imm_error = filt[imm_states] - kpred[imm_states]\n",
    "    imm_error_mean = np.mean(imm_error)\n",
    "    imm_error_std = np.std(imm_error - imm_error_mean)\n",
    "    \n",
    "    # Average value for all states\n",
    "    ca_error_std = np.mean(ca_error_std)\n",
    "    imm_error_std = np.mean(imm_error_std)\n",
    "\n",
    "    return ca_error_std, imm_error_std\n",
    "\n",
    "# Compute the percentage of k-step ahead samples that fall within the band current filtered state +- 1*std\n",
    "def compute_avg_perc(filt, kpred, kpred_var, ca_states, imm_states, ca_variance_idxs, imm_variance_idxs):\n",
    "    ca_filtered_state = filt[ca_states]\n",
    "    \n",
    "    ca_std = kpred_var[ca_variance_idxs].apply(np.sqrt)\n",
    "    ca_std.columns = ca_states\n",
    "    ca_pred_lcl = kpred[ca_states] - 1 * ca_std\n",
    "    ca_pred_ucl = kpred[ca_states] + 1 * ca_std\n",
    "\n",
    "    ca_in_CI_band = (ca_filtered_state >= ca_pred_lcl) & (ca_filtered_state <= ca_pred_ucl)\n",
    "    ca_perc = 100 * np.sum(ca_in_CI_band) / len(filt.dropna())\n",
    "\n",
    "    imm_filtered_state = filt[imm_states]\n",
    "\n",
    "    imm_std = kpred_var[imm_variance_idxs].apply(np.sqrt)\n",
    "    imm_std.columns = imm_states\n",
    "    imm_pred_lcl = kpred[imm_states] - 1 * imm_std\n",
    "    imm_pred_ucl = kpred[imm_states] + 1 * imm_std\n",
    "\n",
    "    imm_in_CI_band = (imm_filtered_state >= imm_pred_lcl) & (imm_filtered_state <= imm_pred_ucl)\n",
    "    imm_perc = 100 * np.sum(imm_in_CI_band) / len(filt.dropna())\n",
    "    \n",
    "    # Average value for all states\n",
    "    ca_perc = np.mean(ca_perc)\n",
    "    imm_perc = np.mean(imm_perc)\n",
    "\n",
    "    return ca_perc, imm_perc\n",
    "    \n",
    "\n",
    "def parameter_tuning(training_subjects, velocities, tasks, pred_horizons, keypoints, dim_per_kpt,\n",
    "                     init_P, var_r, var_q, decrement_factor_q, decrement_factor_P, n_iter_P, n_iter_q,\n",
    "                     n_var_per_dof, n_dim_per_kpt, dim_x, n_kpts,\n",
    "                     var_P_pos, var_P_vel, var_P_acc):\n",
    "    \n",
    "    var_P_acc_init = var_P_acc\n",
    "    \n",
    "    for i in range(n_iter_q):\n",
    "        for j in range(n_iter_P):\n",
    "            print(f\"Iteration {i+1}/{n_iter_q} for var_q and {j+1}/{n_iter_P} for var_P\")\n",
    "            print(f\"Current values: var_q = {var_q}, var_P_acc = {var_P_acc}\")\n",
    "\n",
    "            init_P = initialize_P(n_dim_per_kpt, n_kpts, var_P_pos, var_P_vel, var_P_acc)\n",
    "\n",
    "            _, filtering_results, prediction_results = run_filtering_loop(training_subjects, velocities, tasks, pred_horizons,\n",
    "                                                                        init_P, var_r, var_q)\n",
    "\n",
    "            # Compute the average error between the filtered states and the k-step ahead predictions\n",
    "            avg_errors = {'PICK-&-PLACE': {'CA': 0, 'IMM': 0},\n",
    "                        'WALKING': {'CA': 0, 'IMM': 0},\n",
    "                        'PASSING-BY': {'CA': 0, 'IMM': 0}}\n",
    "\n",
    "            # Compute the average RMSE between the filtered states and the k-step ahead predictions\n",
    "            avg_rmse = copy.deepcopy(avg_errors)\n",
    "            \n",
    "            # Compute the average standard deviation of the filtered states and the k-step ahead predictions\n",
    "            avg_std = copy.deepcopy(avg_errors)\n",
    "            \n",
    "            # Compute the percentage of k-step ahead samples that fall within the band current filtered state +- 1*std\n",
    "            avg_perc = copy.deepcopy(avg_errors)\n",
    "            \n",
    "            for k in pred_horizons:\n",
    "                for subject_id in training_subjects:\n",
    "                    for velocity in velocities:\n",
    "                        for task in tasks:\n",
    "                            filt = filtering_results[(k, subject_id, velocity, task)]['filtered_data']\n",
    "                            kpred = prediction_results[(k, subject_id, velocity, task)]['kstep_pred_data'][k-1]\n",
    "                            kpred_var = prediction_results[(k, subject_id, velocity, task)]['kstep_pred_cov'][k-1]\n",
    "                            \n",
    "                            ca_states = []\n",
    "                            ca_variance_idxs = []\n",
    "                            imm_states = []\n",
    "                            imm_variance_idxs = []\n",
    "                            for kpt in keypoints[task]:\n",
    "                                for dim in dim_per_kpt[kpt]: # ['x', 'y', 'z']:\n",
    "                                    ca_states.append('ca_kp{}_{}'.format(kpt, dim))\n",
    "                                    imm_states.append('imm_kp{}_{}'.format(kpt, dim))\n",
    "\n",
    "                                    state_idx = ['x', 'xd', 'xdd', 'y', 'yd', 'ydd', 'z', 'zd', 'zdd'].index(dim) + n_var_per_dof * n_dim_per_kpt * kpt\n",
    "                                    ca_variance_idx = dim_x * state_idx + state_idx\n",
    "                                    imm_variance_idx = dim_x * state_idx + state_idx + dim_x * dim_x\n",
    "                                    ca_variance_idxs.append(ca_variance_idx)\n",
    "                                    imm_variance_idxs.append(imm_variance_idx)\n",
    "                            \n",
    "                            ca_error, imm_error = compute_mean_error(filt, kpred, ca_states, imm_states)\n",
    "                            ca_rmse, imm_rmse = compute_rmse_error(filt, kpred, ca_states, imm_states)\n",
    "                            ca_std, imm_std = compute_std_error(filt, kpred, ca_states, imm_states)\n",
    "                            ca_perc, imm_perc = compute_avg_perc(filt, kpred, kpred_var,\n",
    "                                                                ca_states, imm_states, ca_variance_idxs, imm_variance_idxs)\n",
    "                            \n",
    "                            avg_errors[task]['CA'] += ca_error\n",
    "                            avg_errors[task]['IMM'] += imm_error\n",
    "                            avg_rmse[task]['CA'] += ca_rmse\n",
    "                            avg_rmse[task]['IMM'] += imm_rmse\n",
    "                            avg_std[task]['CA'] += ca_std\n",
    "                            avg_std[task]['IMM'] += imm_std\n",
    "                            avg_perc[task]['CA'] += ca_perc\n",
    "                            avg_perc[task]['IMM'] += imm_perc\n",
    "\n",
    "            # Compute the average values of these aggregated metrics\n",
    "            num_sums = len(pred_horizons) * len(training_subjects) * len(velocities)\n",
    "            for task in tasks:\n",
    "                avg_errors[task]['CA'] /= num_sums\n",
    "                avg_errors[task]['IMM'] /= num_sums\n",
    "                avg_rmse[task]['CA'] /= num_sums\n",
    "                avg_rmse[task]['IMM'] /= num_sums\n",
    "                avg_std[task]['CA'] /= num_sums\n",
    "                avg_std[task]['IMM'] /= num_sums\n",
    "                avg_perc[task]['CA'] /= num_sums\n",
    "                avg_perc[task]['IMM'] /= num_sums\n",
    "\n",
    "                # Average over all selected keypoints\n",
    "                avg_errors[task]['CA'] = np.mean(avg_errors[task]['CA'])\n",
    "                avg_errors[task]['IMM'] = np.mean(avg_errors[task]['IMM'])\n",
    "                avg_rmse[task]['CA'] = np.mean(avg_rmse[task]['CA'])\n",
    "                avg_rmse[task]['IMM'] = np.mean(avg_rmse[task]['IMM'])\n",
    "                avg_std[task]['CA'] = np.mean(avg_std[task]['CA'])\n",
    "                avg_std[task]['IMM'] = np.mean(avg_std[task]['IMM'])\n",
    "                avg_perc[task]['CA'] = np.mean(avg_perc[task]['CA'])\n",
    "                avg_perc[task]['IMM'] = np.mean(avg_perc[task]['IMM'])\n",
    "\n",
    "            # Display the results aggregating results for all keypoints\n",
    "            print(\"Average error (CA): {:.6f}, {:.6f}, {:.6f}\".format(\n",
    "                avg_errors['PICK-&-PLACE']['CA'], avg_errors['WALKING']['CA'], avg_errors['PASSING-BY']['CA']))\n",
    "            print(\"Average error (IMM): {:.6f}, {:.6f}, {:.6f}\".format(\n",
    "                avg_errors['PICK-&-PLACE']['IMM'], avg_errors['WALKING']['IMM'], avg_errors['PASSING-BY']['IMM']))\n",
    "            print(\"Average RMSE (CA): {:.6f}, {:.6f}, {:.6f}\".format(\n",
    "                avg_rmse['PICK-&-PLACE']['CA'], avg_rmse['WALKING']['CA'], avg_rmse['PASSING-BY']['CA']))\n",
    "            print(\"Average RMSE (IMM): {:.6f}, {:.6f}, {:.6f}\".format(\n",
    "                avg_rmse['PICK-&-PLACE']['IMM'], avg_rmse['WALKING']['IMM'], avg_rmse['PASSING-BY']['IMM']))\n",
    "            print(\"Average std (CA): {:.4f}, {:.4f}, {:.4f}\".format(\n",
    "                avg_std['PICK-&-PLACE']['CA'], avg_std['WALKING']['CA'], avg_std['PASSING-BY']['CA']))\n",
    "            print(\"Average std (IMM): {:.4f}, {:.4f}, {:.4f}\".format(\n",
    "                avg_std['PICK-&-PLACE']['IMM'], avg_std['WALKING']['IMM'], avg_std['PASSING-BY']['IMM']))\n",
    "            print(\"Average percentage (CA): {:.4f}, {:.4f}, {:.4f}\".format(\n",
    "                avg_perc['PICK-&-PLACE']['CA'], avg_perc['WALKING']['CA'], avg_perc['PASSING-BY']['CA']))\n",
    "            print(\"Average percentage (IMM): {:.4f}, {:.4f}, {:.4f}\".format(\n",
    "                avg_perc['PICK-&-PLACE']['IMM'], avg_perc['WALKING']['IMM'], avg_perc['PASSING-BY']['IMM']))\n",
    "            print(\"===============================================\\n\\n\")\n",
    "\n",
    "            # Update the init_P parameter\n",
    "            var_P_acc *= decrement_factor_P\n",
    "\n",
    "        # Update the var_q parameter\n",
    "        var_q *= decrement_factor_q\n",
    "\n",
    "        # Reset the var_P_acc parameter to its initial value\n",
    "        var_P_acc = var_P_acc_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tic = time.time()\n",
    "#train_subjects = train_subjects\n",
    "#velocities = ['FAST'] # worst-case scenario\n",
    "#tasks = TASK_NAMES\n",
    "#pred_horizons = [5] # worst-case scenario\n",
    "#parameter_tuning(train_subjects, ['FAST'], tasks, pred_horizons, keypoints, dimensions_per_keypoint,\n",
    "#                 init_P, var_r, var_q, decrement_factor_q, decrement_factor_P, iter_P, iter_q,\n",
    "#                 n_var_per_dof, n_dim_per_kpt, dim_x, n_kpts,\n",
    "#                 var_P_pos, var_P_vel, var_P_acc)\n",
    "#toc = time.time()\n",
    "\n",
    "#minutes, seconds = divmod(toc - tic, 60)\n",
    "#print(f\"Parameter tuning took {minutes:.0f} minutes and {seconds:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(subjects, velocities, tasks, pred_horizons, keypoints, dimensions_per_keypoint,\n",
    "                 init_P, var_r, var_q, decrement_factor_q, n_var_per_dof, n_dim_per_kpt, dim_x, n_kpts):\n",
    "    \n",
    "    avg_errors = {}\n",
    "    avg_rmse = {}\n",
    "    avg_std = {}\n",
    "    avg_perc = {}\n",
    "    for k in pred_horizons:\n",
    "        _, filtering_results, prediction_results = run_filtering_loop(subjects, velocities, tasks, pred_horizons,\n",
    "                                                                    init_P, var_r, var_q)\n",
    "    \n",
    "        # Compute the average error between the filtered states and the k-step ahead predictions\n",
    "        avg_errors[k] = {'PICK-&-PLACE': {'CA': 0, 'IMM': 0},\n",
    "                    'WALKING': {'CA': 0, 'IMM': 0},\n",
    "                    'PASSING-BY': {'CA': 0, 'IMM': 0}}\n",
    "    \n",
    "        # Compute the average RMSE between the filtered states and the k-step ahead predictions\n",
    "        avg_rmse[k] = copy.deepcopy(avg_errors[k])\n",
    "    \n",
    "        # Compute the average standard deviation of the filtered states and the k-step ahead predictions\n",
    "        avg_std[k] = copy.deepcopy(avg_errors[k])\n",
    "    \n",
    "        # Compute the percentage of k-step ahead samples that fall within the band current filtered state +- 1*std\n",
    "        avg_perc = copy.deepcopy(avg_errors)\n",
    "    \n",
    "        for subject_id in subjects:\n",
    "            for velocity in velocities:\n",
    "                for task in tasks:\n",
    "                    filt = filtering_results[(k, subject_id, velocity, task)]['filtered_data']\n",
    "                    kpred = prediction_results[(k, subject_id, velocity, task)]['kstep_pred_data'][k-1]\n",
    "                    kpred_var = prediction_results[(k, subject_id, velocity, task)]['kstep_pred_cov'][k-1]\n",
    "                    \n",
    "                    ca_states = []\n",
    "                    ca_variance_idxs = []\n",
    "                    imm_states = []\n",
    "                    imm_variance_idxs = []\n",
    "                    for kpt in keypoints[task]:\n",
    "                        for dim in ['x', 'y', 'z']:\n",
    "                            ca_states.append('ca_kp{}_{}'.format(kpt, dim))\n",
    "                            imm_states.append('imm_kp{}_{}'.format(kpt, dim))\n",
    "\n",
    "                            state_idx = ['x', 'xd', 'xdd', 'y', 'yd', 'ydd', 'z', 'zd', 'zdd'].index(dim) + n_var_per_dof * n_dim_per_kpt * kpt\n",
    "                            ca_variance_idx = dim_x * state_idx + state_idx\n",
    "                            imm_variance_idx = dim_x * state_idx + state_idx + dim_x * dim_x\n",
    "                            ca_variance_idxs.append(ca_variance_idx)\n",
    "                            imm_variance_idxs.append(imm_variance_idx)\n",
    "                    \n",
    "                    ca_error, imm_error = compute_mean_error(filt, kpred, ca_states, imm_states)\n",
    "                    ca_rmse, imm_rmse = compute_rmse_error(filt, kpred, ca_states, imm_states)\n",
    "                    ca_std, imm_std = compute_std_error(filt, kpred, ca_states, imm_states)\n",
    "                    ca_perc, imm_perc = compute_avg_perc(filt, kpred, kpred_var,\n",
    "                                                        ca_states, imm_states, ca_variance_idxs, imm_variance_idxs)\n",
    "                    \n",
    "                    avg_errors[k][task]['CA'] += ca_error\n",
    "                    avg_errors[k][task]['IMM'] += imm_error\n",
    "                    avg_rmse[k][task]['CA'] += ca_rmse\n",
    "                    avg_rmse[k][task]['IMM'] += imm_rmse\n",
    "                    avg_std[k][task]['CA'] += ca_std\n",
    "                    avg_std[k][task]['IMM'] += imm_std\n",
    "                    avg_perc[k][task]['CA'] += ca_perc\n",
    "                    avg_perc[k][task]['IMM'] += imm_perc\n",
    "\n",
    "        # Compute the average values of these aggregated metrics\n",
    "        num_sums = len(pred_horizons) * len(subjects) * len(velocities)\n",
    "        for task in tasks:\n",
    "            avg_errors[k][task]['CA'] /= num_sums\n",
    "            avg_errors[k][task]['IMM'] /= num_sums\n",
    "            avg_rmse[k][task]['CA'] /= num_sums\n",
    "            avg_rmse[k][task]['IMM'] /= num_sums\n",
    "            avg_std[k][task]['CA'] /= num_sums\n",
    "            avg_std[k][task]['IMM'] /= num_sums\n",
    "            avg_perc[k][task]['CA'] /= num_sums\n",
    "            avg_perc[k][task]['IMM'] /= num_sums\n",
    "    \n",
    "            # Average over all selected keypoints\n",
    "            avg_errors[k][task]['CA'] = np.mean(avg_errors[task]['CA'])\n",
    "            avg_errors[k][task]['IMM'] = np.mean(avg_errors[task]['IMM'])\n",
    "            avg_rmse[k][task]['CA'] = np.mean(avg_rmse[task]['CA'])\n",
    "            avg_rmse[k][task]['IMM'] = np.mean(avg_rmse[task]['IMM'])\n",
    "            avg_std[k][task]['CA'] = np.mean(avg_std[task]['CA'])\n",
    "            avg_std[k][task]['IMM'] = np.mean(avg_std[task]['IMM'])\n",
    "            avg_perc[k][task]['CA'] = np.mean(avg_perc[task]['CA'])\n",
    "            avg_perc[k][task]['IMM'] = np.mean(avg_perc[task]['IMM'])\n",
    "    \n",
    "        # Display the results aggregating results for all keypoints\n",
    "        print(f\"Results for k={k}\")\n",
    "        print(\"Average error (CA): {:.6f}, {:.6f}, {:.6f}\".format(\n",
    "            avg_errors[k]['PICK-&-PLACE']['CA'], avg_errors[k]['WALKING']['CA'], avg_errors[k]['PASSING-BY']['CA']))\n",
    "        print(\"Average error (IMM): {:.6f}, {:.6f}, {:.6f}\".format(\n",
    "            avg_errors[k]['PICK-&-PLACE']['IMM'], avg_errors[k]['WALKING']['IMM'], avg_errors[k]['PASSING-BY']['IMM']))\n",
    "        print(\"Average RMSE (CA): {:.6f}, {:.6f}, {:.6f}\".format(\n",
    "            avg_rmse[k]['PICK-&-PLACE']['CA'], avg_rmse[k]['WALKING']['CA'], avg_rmse[k]['PASSING-BY']['CA']))\n",
    "        print(\"Average RMSE (IMM): {:.6f}, {:.6f}, {:.6f}\".format(\n",
    "            avg_rmse[k]['PICK-&-PLACE']['IMM'], avg_rmse[k]['WALKING']['IMM'], avg_rmse[k]['PASSING-BY']['IMM']))\n",
    "        print(\"Average std (CA): {:.4f}, {:.4f}, {:.4f}\".format(\n",
    "            avg_std[k]['PICK-&-PLACE']['CA'], avg_std[k]['WALKING']['CA'], avg_std[k]['PASSING-BY']['CA']))\n",
    "        print(\"Average std (IMM): {:.4f}, {:.4f}, {:.4f}\".format(\n",
    "            avg_std[k]['PICK-&-PLACE']['IMM'], avg_std[k]['WALKING']['IMM'], avg_std[k]['PASSING-BY']['IMM']))\n",
    "        print(\"Average percentage (CA): {:.4f}, {:.4f}, {:.4f}\".format(\n",
    "            avg_perc[k]['PICK-&-PLACE']['CA'], avg_perc[k]['WALKING']['CA'], avg_perc[k]['PASSING-BY']['CA']))\n",
    "        print(\"Average percentage (IMM): {:.4f}, {:.4f}, {:.4f}\".format(\n",
    "            avg_perc[k]['PICK-&-PLACE']['IMM'], avg_perc[k]['WALKING']['IMM'], avg_perc[k]['PASSING-BY']['IMM']))\n",
    "        print(\"===============================================\\n\\n\")\n",
    "\n",
    "        # Export results to a CSV file\n",
    "        results = {'CA_error': [avg_errors[k]['PICK-&-PLACE']['CA'], avg_errors[k]['WALKING']['CA'], avg_errors[k]['PASSING-BY']['CA']],\n",
    "                    'IMM_error': [avg_errors[k]['PICK-&-PLACE']['IMM'], avg_errors[k]['WALKING']['IMM'], avg_errors[k]['PASSING-BY']['IMM']],\n",
    "                    'CA_RMSE': [avg_rmse[k]['PICK-&-PLACE']['CA'], avg_rmse[k]['WALKING']['CA'], avg_rmse[k]['PASSING-BY']['CA']],\n",
    "                    'IMM_RMSE': [avg_rmse[k]['PICK-&-PLACE']['IMM'], avg_rmse[k]['WALKING']['IMM'], avg_rmse[k]['PASSING-BY']['IMM']],\n",
    "                    'CA_std': [avg_std[k]['PICK-&-PLACE']['CA'], avg_std[k]['WALKING']['CA'], avg_std[k]['PASSING-BY']['CA']],\n",
    "                    'IMM_std': [avg_std[k]['PICK-&-PLACE']['IMM'], avg_std[k]['WALKING']['IMM'], avg_std[k]['PASSING-BY']['IMM']],\n",
    "                    'CA_perc': [avg_perc[k]['PICK-&-PLACE']['CA'], avg_perc[k]['WALKING']['CA'], avg_perc[k]['PASSING-BY']['CA']],\n",
    "                    'IMM_perc': [avg_perc[k]['PICK-&-PLACE']['IMM'], avg_perc[k]['WALKING']['IMM'], avg_perc[k]['PASSING-BY']['IMM']]}\n",
    "        \n",
    "        results_df = pd.DataFrame(results, index=['PICK-&-PLACE', 'WALKING', 'PASSING-BY'])\n",
    "        results_df.to_csv(f'results_{k}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "train_subjects = train_subjects\n",
    "velocities = VELOCITIES\n",
    "tasks = TASK_NAMES\n",
    "pred_horizons = PRED_HORIZONS\n",
    "\n",
    "keypoints = {'PICK-&-PLACE': [4, 7], # [2, 3, 4, 5, 6, 7],\n",
    "             'WALKING': [0, 1], # [0, 1, 2, 5, 8, 11],\n",
    "             'PASSING-BY': [0, 1]} # [0, 1, 2, 5, 8, 11]}\n",
    "\n",
    "dimensions_per_keypoint = {0: ['y'],\n",
    "                           1: ['y'],\n",
    "                           4: ['x', 'y', 'z'],\n",
    "                           7: ['x', 'y', 'z']}\n",
    "\n",
    "# Initial values for the parameters\n",
    "var_r = 0.0025          # [paper: r_y] Hip: 3-sigma (99.5%) = 0.15 m ==> sigma 0.05 m ==> var = (0.05)^2 m^2\n",
    "var_q = 0.01            # [paper: q_a] a_dot = u (u = 0 is very uncertain ==> add variance here)\n",
    "var_P_pos = var_r       # [paper: p_y] Set equal to the measurement noise since the state is initialized with the measurement\n",
    "var_P_vel = 0.02844     # [paper: p_v] Hip: no keypoint moves faster than 1.6 m/s ==> 3-sigma (99.5%) = 1.6 m/s ==> var = (1.6/3)^2 m^2/s^2\n",
    "var_P_acc = 1.1111      # [paper: p_a] Hip: no keypoint accelerates faster than 10 m/s^2 ==> 3-sigma (99.5%) = 10 m/s^2 ==> var = (10/3)^2 m^2/s^4\n",
    "init_P = initialize_P(n_dim_per_kpt, n_kpts, var_P_pos, var_P_vel, var_P_acc)\n",
    "\n",
    "evaluate_metrics(train_subjects, velocities, tasks, pred_horizons, keypoints, dimensions_per_keypoint,\n",
    "                 init_P, var_r, var_q, decrement_factor_q, n_var_per_dof, n_dim_per_kpt, dim_x, n_kpts)\n",
    "\n",
    "toc = time.time()\n",
    "minutes, seconds = divmod(toc - tic, 60)\n",
    "print(f\"[IDENTIFICATION] Metrics evaluation took {minutes:.0f} minutes and {seconds:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
